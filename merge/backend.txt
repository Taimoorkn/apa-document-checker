// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\analyzers\ApaAnalyzer.js
// server/analyzers/ApaAnalyzer.js - Server-side APA analyzer (CommonJS)
// Placeholder implementation - for fallback

/**
 * Server-side APA 7th Edition Analyzer
 * Analyzes document data and returns compliance issues
 */
class ApaAnalyzer {
  constructor() {
    this.apaStandards = {
      font: {
        family: 'times new roman',
        size: 12 // points
      },
      spacing: {
        line: 2.0, // double spacing
        paragraphAfter: 0,
        paragraphBefore: 0
      },
      margins: {
        top: 1.0, // inches
        bottom: 1.0,
        left: 1.0,
        right: 1.0
      },
      indentation: {
        firstLine: 0.5 // inches
      }
    };
  }

  /**
   * Analyze document for APA compliance
   * @param {Object} documentData - Document data from XmlDocxProcessor
   * @returns {Array} Array of compliance issues
   */
  analyzeDocument(documentData) {
    const issues = [];

    // Validate input
    if (!documentData) {
      console.error('‚ùå analyzeDocument called with null/undefined documentData');
      return issues;
    }

    const {
      text = '',
      html = '',
      formatting = null,
      structure = null
    } = documentData;

    // Validate required fields
    if (!text && !html) {
      console.error('‚ùå Document has no text or HTML content');
      return issues;
    }

    console.log(`üìä Analyzing document: ${text.length} chars, ${formatting?.paragraphs?.length || 0} paragraphs`);

    try {
      // Run all validators
      this.validateFormatting(formatting, issues);
      this.validateStructure(structure, text, issues);
      this.validateReferences(text, issues);
      this.validateCitations(text, issues);

    } catch (error) {
      console.error('Error during analysis:', error);
      issues.push({
        id: `analysis-error-${Date.now()}`,
        title: 'Analysis Error',
        description: 'An error occurred during document analysis',
        severity: 'Critical',
        category: 'document',
        hasFix: false,
        explanation: error.message
      });
    }

    console.log(`‚úÖ Analysis complete: ${issues.length} issues found`);
    return issues;
  }

  /**
   * Validate document formatting (font, spacing, margins)
   */
  validateFormatting(formatting, issues) {
    if (!formatting || !formatting.document) {
      return;
    }

    const doc = formatting.document;

    // Check font family
    if (doc.font && doc.font.family) {
      const fontFamily = doc.font.family.toLowerCase();
      if (!fontFamily.includes('times') && !fontFamily.includes('arial') &&
          !fontFamily.includes('calibri') && !fontFamily.includes('georgia')) {
        issues.push({
          id: `font-family-${Date.now()}`,
          title: 'Non-APA Compliant Font',
          description: `Document uses "${doc.font.family}" font. APA recommends Times New Roman, Arial, Calibri, or Georgia.`,
          severity: 'Major',
          category: 'Formatting',
          hasFix: true,
          fixAction: 'fixFont',
          fixValue: 'Times New Roman',
          explanation: 'APA 7th edition recommends accessible fonts like Times New Roman 12pt.'
        });
      }
    }

    // Check font size
    if (doc.font && doc.font.size) {
      if (doc.font.size < 11 || doc.font.size > 12) {
        issues.push({
          id: `font-size-${Date.now()}`,
          title: 'Non-Standard Font Size',
          description: `Document uses ${doc.font.size}pt font. APA recommends 12pt.`,
          severity: 'Major',
          category: 'Formatting',
          hasFix: true,
          fixAction: 'fixFontSize',
          fixValue: '12pt',
          explanation: 'APA 7th edition requires 12-point font size for body text.'
        });
      }
    }

    // Check line spacing
    if (doc.spacing && doc.spacing.line) {
      if (doc.spacing.line < 1.9 || doc.spacing.line > 2.1) {
        issues.push({
          id: `line-spacing-${Date.now()}`,
          title: 'Incorrect Line Spacing',
          description: `Document uses ${doc.spacing.line} line spacing. APA requires double spacing (2.0).`,
          severity: 'Major',
          category: 'Formatting',
          hasFix: true,
          fixAction: 'fixLineSpacing',
          fixValue: '2.0',
          explanation: 'APA 7th edition requires double-spaced lines throughout the document.'
        });
      }
    }

    // Check margins
    if (doc.margins) {
      const marginIssues = [];
      if (doc.margins.top && doc.margins.top < 0.9) marginIssues.push('top');
      if (doc.margins.bottom && doc.margins.bottom < 0.9) marginIssues.push('bottom');
      if (doc.margins.left && doc.margins.left < 0.9) marginIssues.push('left');
      if (doc.margins.right && doc.margins.right < 0.9) marginIssues.push('right');

      if (marginIssues.length > 0) {
        issues.push({
          id: `margins-${Date.now()}`,
          title: 'Insufficient Margins',
          description: `Document has insufficient ${marginIssues.join(', ')} margins. APA requires 1-inch margins on all sides.`,
          severity: 'Major',
          category: 'Formatting',
          hasFix: true,
          fixAction: 'fixMargins',
          fixValue: '1 inch',
          explanation: 'APA 7th edition requires 1-inch margins on all sides of the page.'
        });
      }
    }
  }

  /**
   * Validate document structure (headings, sections)
   */
  validateStructure(structure, text, issues) {
    if (!structure) {
      return;
    }

    // Check for headings
    if (structure.headings && structure.headings.length > 0) {
      // Validate heading levels
      let previousLevel = 0;
      structure.headings.forEach((heading, index) => {
        const level = heading.level || 1;

        // Check for skipped levels
        if (level - previousLevel > 1) {
          issues.push({
            id: `heading-level-skip-${index}`,
            title: 'Heading Level Skipped',
            description: `Heading "${heading.text}" skips from level ${previousLevel} to level ${level}. APA requires sequential heading levels.`,
            severity: 'Minor',
            category: 'Structure',
            hasFix: false,
            explanation: 'APA 7th edition requires heading levels to be used sequentially (Level 1, then Level 2, etc.).'
          });
        }

        previousLevel = level;
      });
    }
  }

  /**
   * Validate references section
   */
  validateReferences(text, issues) {
    // Check for "References" section
    const referencesMatch = text.match(/\b(References?|Bibliography|Works Cited)\b/i);

    if (!referencesMatch) {
      issues.push({
        id: `missing-references-${Date.now()}`,
        title: 'Missing References Section',
        description: 'No "References" section found. APA papers must include a references page.',
        severity: 'Critical',
        category: 'References',
        hasFix: false,
        explanation: 'APA 7th edition requires a "References" section listing all cited sources.'
      });
      return;
    }

    // Check for proper heading (should be "References" not "Bibliography")
    if (referencesMatch[1].toLowerCase() !== 'references') {
      issues.push({
        id: `wrong-references-heading-${Date.now()}`,
        title: 'Incorrect References Heading',
        description: `Section is titled "${referencesMatch[1]}" but should be "References" in APA style.`,
        severity: 'Major',
        category: 'References',
        hasFix: false,
        explanation: 'APA 7th edition requires the heading "References" (not "Bibliography" or "Works Cited").'
      });
    }
  }

  /**
   * Validate in-text citations
   */
  validateCitations(text, issues) {
    // Check for basic citation patterns
    const citationPattern = /\([^)]*\d{4}[^)]*\)/g;
    const citations = text.match(citationPattern) || [];

    if (citations.length === 0) {
      issues.push({
        id: `no-citations-${Date.now()}`,
        title: 'No In-Text Citations Found',
        description: 'No in-text citations detected. Academic papers should cite sources.',
        severity: 'Major',
        category: 'Citations',
        hasFix: false,
        explanation: 'APA 7th edition requires in-text citations for all referenced sources in the format (Author, Year).'
      });
    }
  }

  /**
   * Calculate compliance score based on issues
   */
  calculateComplianceScore(issues) {
    if (!issues || issues.length === 0) {
      return 100;
    }

    // Deduct points based on severity
    let totalDeduction = 0;
    issues.forEach(issue => {
      switch (issue.severity) {
        case 'Critical':
          totalDeduction += 15;
          break;
        case 'Major':
          totalDeduction += 8;
          break;
        case 'Minor':
          totalDeduction += 3;
          break;
        default:
          totalDeduction += 5;
      }
    });

    // Cap at 0 and 100
    const score = Math.max(0, Math.min(100, 100 - totalDeduction));
    return Math.round(score);
  }
}

module.exports = ApaAnalyzer;


// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\index.js
// server/index.js - Compatible with both traditional and Vercel serverless deployment
require('dotenv').config(); // Load environment variables from root .env

const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const path = require('path');
const fs = require('fs').promises;

// Create app instance
const app = express();
const PORT = process.env.PORT || 3001;

// Security middleware
app.use(helmet({
  crossOriginEmbedderPolicy: false, // Needed for file uploads
}));

// CORS configuration - Updated for Vercel deployment
app.use(cors({
  origin: process.env.NODE_ENV === 'production'
    ? true // Allow all origins in production (Vercel handles this)
    : ['http://localhost:3000', 'http://127.0.0.1:3000'],
  credentials: true,
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowedHeaders: ['Content-Type', 'Authorization', 'Accept']
}));

// Body parsing middleware
app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// Simple logging for serverless environments
const simpleLogger = (req, res, next) => {
  console.log(`${new Date().toISOString()} - ${req.method} ${req.path}`);
  next();
};
app.use(simpleLogger);

// Create uploads directory if it doesn't exist (for local development)
if (process.env.NODE_ENV !== 'production') {
  const uploadsDir = path.join(__dirname, 'uploads');
  fs.mkdir(uploadsDir, { recursive: true }).catch(error =>
    console.error('Failed to create uploads directory:', error)
  );
}

// Health check endpoint with worker pool stats
app.get('/api/health', (req, res) => {
  // Get worker pool stats if available
  let workerPoolStats = null;
  try {
    const docxRoutes = require('./routes/docx');
    if (docxRoutes.workerPool) {
      const stats = docxRoutes.workerPool.getStats();
      workerPoolStats = {
        enabled: true,
        poolSize: stats.poolSize,
        availableWorkers: stats.availableWorkers,
        busyWorkers: stats.busyWorkers,
        activeJobs: stats.activeJobs,
        totalJobsProcessed: stats.totalJobsProcessed
      };
    }
  } catch (error) {
    // Worker pool not available (serverless or not initialized)
  }

  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    service: 'APA Document Checker Server',
    environment: process.env.NODE_ENV || 'development',
    platform: process.env.VERCEL ? 'vercel' : 'traditional',
    workerPool: workerPoolStats || { enabled: false }
  });
});

// Import and use document processing routes
const docxRoutes = require('./routes/docx');
app.use('/api', docxRoutes);

// Error handling middleware
app.use((error, req, res, next) => {
  console.error('Server error:', error);

  // Handle multer errors (file upload errors)
  if (error.code === 'LIMIT_FILE_SIZE') {
    return res.status(413).json({
      success: false,
      error: 'File too large. Maximum size is 10MB.',
      code: 'FILE_TOO_LARGE'
    });
  }

  if (error.code === 'LIMIT_UNEXPECTED_FILE') {
    return res.status(400).json({
      success: false,
      error: 'Unexpected file field. Please upload a DOCX file.',
      code: 'UNEXPECTED_FILE'
    });
  }

  // Handle other errors
  res.status(500).json({
    success: false,
    error: 'Internal server error',
    message: process.env.NODE_ENV === 'development' ? error.message : 'Something went wrong',
    code: 'INTERNAL_ERROR'
  });
});

// 404 handler for API routes
app.use('/api/*', (req, res) => {
  res.status(404).json({
    success: false,
    error: 'API endpoint not found',
    code: 'NOT_FOUND'
  });
});

// Only start server if not in serverless environment (Vercel)
if (!process.env.VERCEL) {
  app.listen(PORT, (err) => {
    if (err) {
      console.error('Failed to start server:', err);
      process.exit(1);
    }

    console.log(`Server started successfully on port ${PORT}`);
    console.log(`Environment: ${process.env.NODE_ENV || 'development'}`);
  });

  // Graceful shutdown for traditional deployment
  process.on('SIGTERM', () => {
    console.log('SIGTERM received, shutting down gracefully');
    process.exit(0);
  });

  process.on('SIGINT', () => {
    console.log('SIGINT received, shutting down gracefully');
    process.exit(0);
  });
}

module.exports = app;



// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\processors\DocxModifier.js
// server/processors/DocxModifier.js - DOCX modification for APA fixes (Memory-based)
const PizZip = require('pizzip');
const { DOMParser, XMLSerializer } = require('@xmldom/xmldom');

class DocxModifier {
  constructor() {
    this.supportedFixes = [
      // Formatting fixes (DOCX modification)
      'fixFont', 'fixFontSize', 'fixLineSpacing', 'fixMargins', 'fixIndentation',
      // Content fixes (text replacement in XML)
      'addCitationComma', 'fixParentheticalConnector', 'fixEtAlFormatting', 
      'fixReferenceConnector', 'fixAllCapsHeading', 'addPageNumber'
    ];
  }

  /**
   * Apply formatting fix to a DOCX buffer by modifying the document structure
   */
  async applyFormattingFix(inputBuffer, fixAction, fixValue) {
    try {
      console.log(`üîß DocxModifier.applyFormattingFix called with:`, {
        bufferSize: inputBuffer?.length,
        fixAction,
        fixValueType: typeof fixValue,
        fixValueKeys: typeof fixValue === 'object' ? Object.keys(fixValue || {}) : 'N/A'
      });
      
      if (!inputBuffer || inputBuffer.length === 0) {
        throw new Error('Invalid or empty input buffer provided');
      }
      
      // Create zip from buffer
      console.log('üì¶ Creating PizZip from buffer...');
      const zip = new PizZip(inputBuffer);
      console.log('‚úÖ PizZip created successfully');
      
      // Apply the specific fix by modifying document XML
      const modifiedZip = await this.modifyDocumentXML(zip, fixAction, fixValue);
      
      // Generate the modified DOCX buffer
      const outputBuffer = modifiedZip.generate({
        type: 'nodebuffer',
        compression: 'DEFLATE'
      });
      
      return { success: true, buffer: outputBuffer };
      
    } catch (error) {
      console.error('Error modifying DOCX:', error);
      return { success: false, error: error.message };
    }
  }

  /**
   * Modify the document XML to apply formatting fixes
   */
  async modifyDocumentXML(zip, fixAction, fixValue) {
    try {
      console.log(`üîÑ ModifyDocumentXML called with fixAction: ${fixAction}`);
      
      // Get the main document XML
      const docXmlFile = zip.file('word/document.xml');
      if (!docXmlFile) {
        throw new Error('Could not find document.xml in DOCX file');
      }
      
      console.log('üìÑ Found document.xml, reading content...');
      // Read document content using the correct PizZip API
      let documentContent = docXmlFile.asText();
      console.log(`üìÑ Document XML content length: ${documentContent.length} characters`);
      
      // Apply specific formatting fixes
      switch (fixAction) {
        case 'fixFont':
          documentContent = this.fixFontFamily(documentContent, fixValue || 'Times New Roman');
          break;
          
        case 'fixFontSize':
          documentContent = this.fixFontSize(documentContent, fixValue || 24); // 24 half-points = 12pt
          break;
          
        case 'fixLineSpacing':
          documentContent = this.fixLineSpacing(documentContent, fixValue || 480); // 480 = double spacing
          break;

        // Text-based content fixes
        case 'addCitationComma':
          documentContent = this.fixTextContent(documentContent, fixValue);
          break;
          
        case 'fixParentheticalConnector':
          documentContent = this.fixTextContent(documentContent, fixValue);
          break;
          
        case 'fixEtAlFormatting':
          documentContent = this.fixTextContent(documentContent, fixValue);
          break;
          
        case 'fixReferenceConnector':
          documentContent = this.fixTextContent(documentContent, fixValue);
          break;
          
        case 'fixAllCapsHeading':
          documentContent = this.fixTextContent(documentContent, fixValue);
          break;
          
        case 'addPageNumber':
          documentContent = this.fixTextContent(documentContent, fixValue);
          break;
          
        default:
          console.warn(`Unsupported fix action: ${fixAction}`);
          return zip; // Return unchanged if fix not supported
      }
      
      // Update the document XML in the zip
      zip.file('word/document.xml', documentContent);
      
      // Also modify styles.xml if it exists for document-wide changes
      const stylesXmlFile = zip.file('word/styles.xml');
      if (stylesXmlFile && ['fixFont', 'fixFontSize', 'fixLineSpacing'].includes(fixAction)) {
        let stylesContent = stylesXmlFile.asText();
        stylesContent = this.modifyStyles(stylesContent, fixAction, fixValue);
        zip.file('word/styles.xml', stylesContent);
      }
      
      return zip;
      
    } catch (error) {
      console.error('Error modifying document XML:', error);
      throw error;
    }
  }

  /**
   * Fix font family in document XML with proper escaping
   */
  fixFontFamily(xmlContent, fontFamily) {
    // Escape the font family for XML safety
    const escapedFontFamily = this.escapeXml(fontFamily);
    
    // Replace all font family references in run properties
    // Pattern matches: <w:rFonts w:ascii="..." w:hAnsi="..." ... />
    xmlContent = xmlContent.replace(
      /<w:rFonts[^>]*w:ascii="[^"]*"([^>]*)/g,
      `<w:rFonts w:ascii="${escapedFontFamily}"$1`
    );
    
    xmlContent = xmlContent.replace(
      /<w:rFonts[^>]*w:hAnsi="[^"]*"([^>]*)/g,
      (match, rest) => match.replace(/w:hAnsi="[^"]*"/, `w:hAnsi="${escapedFontFamily}"`)
    );
    
    // Add font family to runs that don't have it
    xmlContent = xmlContent.replace(
      /<w:rPr>(?![^<]*<w:rFonts)/g,
      `<w:rPr><w:rFonts w:ascii="${escapedFontFamily}" w:hAnsi="${escapedFontFamily}"/>`
    );
    
    return xmlContent;
  }

  /**
   * Fix font size in document XML
   */
  fixFontSize(xmlContent, halfPoints) {
    
    // Replace existing font size declarations
    xmlContent = xmlContent.replace(
      /<w:sz w:val="\d+"/g,
      `<w:sz w:val="${halfPoints}"`
    );
    
    xmlContent = xmlContent.replace(
      /<w:szCs w:val="\d+"/g,
      `<w:szCs w:val="${halfPoints}"`
    );
    
    // Add font size to runs that don't have it
    xmlContent = xmlContent.replace(
      /<w:rPr>(?![^<]*<w:sz)/g,
      `<w:rPr><w:sz w:val="${halfPoints}"/><w:szCs w:val="${halfPoints}"/>`
    );
    
    // Handle cases where rPr exists but doesn't have size
    xmlContent = xmlContent.replace(
      /<w:rPr>([^<]*(?:<w:(?!sz)[^>]*>[^<]*<\/w:[^>]*>)*[^<]*)<\/w:rPr>/g,
      (match, content) => {
        if (!content.includes('<w:sz')) {
          return `<w:rPr>${content}<w:sz w:val="${halfPoints}"/><w:szCs w:val="${halfPoints}"/></w:rPr>`;
        }
        return match;
      }
    );
    
    return xmlContent;
  }

  /**
   * Fix line spacing in document XML
   */
  fixLineSpacing(xmlContent, spacing) {
    
    // Replace existing spacing declarations in paragraph properties
    xmlContent = xmlContent.replace(
      /<w:spacing[^>]*w:line="\d+"[^>]*\/>/g,
      `<w:spacing w:line="${spacing}" w:lineRule="auto"/>`
    );
    
    // Add spacing to paragraphs that don't have it
    xmlContent = xmlContent.replace(
      /<w:pPr>(?![^<]*<w:spacing)/g,
      `<w:pPr><w:spacing w:line="${spacing}" w:lineRule="auto"/>`
    );
    
    return xmlContent;
  }

  /**
   * Modify styles.xml for document-wide formatting changes
   */
  modifyStyles(stylesContent, fixAction, fixValue) {
    
    try {
      switch (fixAction) {
        case 'fixFont':
          // Update the Normal style and other paragraph styles
          stylesContent = stylesContent.replace(
            /<w:rFonts[^>]*w:ascii="[^"]*"/g,
            `<w:rFonts w:ascii="${fixValue}"`
          );
          stylesContent = stylesContent.replace(
            /<w:rFonts[^>]*w:hAnsi="[^"]*"/g,
            (match) => match.replace(/w:hAnsi="[^"]*"/, `w:hAnsi="${fixValue}"`)
          );
          break;
          
        case 'fixFontSize':
          // Update default font sizes in styles
          stylesContent = stylesContent.replace(
            /<w:sz w:val="\d+"/g,
            `<w:sz w:val="${fixValue}"`
          );
          stylesContent = stylesContent.replace(
            /<w:szCs w:val="\d+"/g,
            `<w:szCs w:val="${fixValue}"`
          );
          break;
          
        case 'fixLineSpacing':
          // Update line spacing in paragraph styles
          stylesContent = stylesContent.replace(
            /<w:spacing[^>]*w:line="\d+"[^>]*\/>/g,
            `<w:spacing w:line="${fixValue}" w:lineRule="auto"/>`
          );
          break;
      }
      
      return stylesContent;
      
    } catch (error) {
      console.error('Error modifying styles:', error);
      return stylesContent; // Return unchanged if error
    }
  }

  /**
   * Fix text content in document XML using proper DOM manipulation (for citation and content fixes)
   */
  fixTextContent(xmlContent, fixValue) {
    // Validate inputs first - support both property name formats
    if (!fixValue) {
      console.warn('Invalid fixValue for text content fix: null or undefined');
      return xmlContent;
    }

    // Support both naming conventions: originalText/replacementText OR original/replacement
    const originalText = fixValue.originalText || fixValue.original;
    const replacementText = fixValue.replacementText || fixValue.replacement;

    if (!originalText || !replacementText) {
      console.warn('Invalid fixValue for text content fix - missing required properties:', {
        fixValue,
        hasOriginalText: !!fixValue.originalText,
        hasOriginal: !!fixValue.original,
        hasReplacementText: !!fixValue.replacementText,
        hasReplacement: !!fixValue.replacement
      });
      return xmlContent;
    }

    try {
      console.log(`üîÑ Replacing text in XML using DOM: "${originalText}" ‚Üí "${replacementText}"`);

      // Parse XML with proper DOM parser
      const parser = new DOMParser();
      const xmlDoc = parser.parseFromString(xmlContent, 'text/xml');

      if (!xmlDoc || xmlDoc.getElementsByTagName('parsererror').length > 0) {
        console.warn('‚ö†Ô∏è XML parsing failed, falling back to safe regex replacement');
        return this.safeFallbackTextReplacement(xmlContent, originalText, replacementText);
      }

      // Find all w:t (text) elements
      const textElements = xmlDoc.getElementsByTagName('w:t');
      let replacementsMade = 0;

      console.log(`üîç Searching for "${originalText}" in ${textElements.length} text elements...`);

      // Process each text element
      for (let i = 0; i < textElements.length; i++) {
        const textElement = textElements[i];
        const textContent = textElement.textContent || '';

        if (textContent.includes(originalText)) {
          console.log(`‚úÖ MATCH FOUND at element ${i}: "${textContent}"`);

          // Replace text content while preserving XML structure
          const newTextContent = textContent.replace(new RegExp(this.escapeRegex(originalText), 'g'), replacementText);

          console.log(`üìù Replacing with: "${newTextContent}"`);

          // Clear existing text nodes and create new one
          while (textElement.firstChild) {
            textElement.removeChild(textElement.firstChild);
          }

          textElement.appendChild(xmlDoc.createTextNode(newTextContent));
          replacementsMade++;
        }
      }

      if (replacementsMade > 0) {
        // Serialize back to XML
        const serializer = new XMLSerializer();
        const modifiedXml = serializer.serializeToString(xmlDoc);

        console.log(`‚úÖ DOM-based text replacement successful: ${replacementsMade} replacement(s) made`);
        console.log(`üìù Replaced: "${originalText}" ‚Üí "${replacementText}"`);

        return modifiedXml;
      } else {
        console.warn('‚ö†Ô∏è No text replacement occurred - text not found in w:t elements');
        return xmlContent;
      }

    } catch (error) {
      console.error('Error in DOM-based fixTextContent:', error);
      console.warn('‚ö†Ô∏è Falling back to safe regex replacement');
      return this.safeFallbackTextReplacement(xmlContent, originalText, replacementText);
    }
  }

  /**
   * Safe fallback text replacement using regex with proper escaping
   */
  safeFallbackTextReplacement(xmlContent, originalText, replacementText) {
    try {
      // Only replace text within w:t elements to avoid breaking XML structure
      const xmlSafeReplacement = this.escapeXml(replacementText);
      const escapedOriginal = this.escapeRegex(originalText);

      // Replace text within w:t tags only
      const replaced = xmlContent.replace(
        new RegExp(`(<w:t[^>]*>)([^<]*?)(${escapedOriginal})([^<]*?)(</w:t>)`, 'g'),
        (match, openTag, beforeText, matchedText, afterText, closeTag) => {
          const newTextContent = beforeText + xmlSafeReplacement + afterText;
          return openTag + newTextContent + closeTag;
        }
      );

      if (replaced !== xmlContent) {
        console.log('‚úÖ Safe fallback text replacement successful');
        return replaced;
      }

      return xmlContent;
    } catch (error) {
      console.error('Error in safe fallback text replacement:', error);
      return xmlContent;
    }
  }

  /**
   * DEPRECATED: applyTextChanges - REMOVED
   * This method was fundamentally flawed (regex-based XML manipulation).
   *
   * NEW ARCHITECTURE:
   * - Manual edits are saved as Tiptap JSON to Supabase
   * - DOCX is generated fresh on export only (not incrementally modified)
   * - See DocxGenerator.js for export functionality
   *
   * This stub remains for backward compatibility during migration.
   */
  async applyTextChanges(inputBuffer, paragraphs) {
    console.warn('‚ö†Ô∏è applyTextChanges is deprecated - use JSON-based auto-save instead');
    return {
      success: false,
      error: 'This method is deprecated. Use JSON-based architecture.',
      deprecated: true
    };
  }

  /**
   * Escape special regex characters
   */
  escapeRegex(text) {
    if (!text || typeof text !== 'string') {
      return '';
    }
    return text.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
  }

  /**
   * Escape XML special characters
   */
  escapeXml(text) {
    return text
      .replace(/&/g, '&amp;')
      .replace(/</g, '&lt;')
      .replace(/>/g, '&gt;')
      .replace(/"/g, '&quot;')
      .replace(/'/g, '&#39;');
  }

  /**
   * Check if a fix action is supported
   */
  isFixSupported(fixAction) {
    return this.supportedFixes.includes(fixAction);
  }
}

module.exports = DocxModifier;

// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\processors\XmlDocxProcessor.js
// server/processors/XmlDocxProcessor.js - XML-based DOCX processor using xml2js and PizZip
const PizZip = require('pizzip');
const xml2js = require('xml2js');
const fs = require('fs').promises;
const path = require('path');

class XmlDocxProcessor {
  // Static tracking of temporary files for emergency cleanup
  static tempFiles = new Set();
  static cleanupRegistered = false;

  constructor() {
    this.parser = new xml2js.Parser({
      explicitArray: false,
      ignoreAttrs: false,
      mergeAttrs: false,
      explicitCharkey: true
    });

    // APA formatting standards for comparison
    this.apaStandards = {
      font: { family: 'Times New Roman', size: 12 },
      spacing: { line: 2.0 },
      margins: { top: 1.0, bottom: 1.0, left: 1.0, right: 1.0 },
      indentation: { firstLine: 0.5 }
    };

    // Register process cleanup handlers (only once)
    if (!XmlDocxProcessor.cleanupRegistered) {
      this.registerCleanupHandlers();
      XmlDocxProcessor.cleanupRegistered = true;
    }
  }

  // Register cleanup handlers for process exit events
  registerCleanupHandlers() {
    const cleanup = async () => {
      if (XmlDocxProcessor.tempFiles.size > 0) {
        console.log(`Cleaning up ${XmlDocxProcessor.tempFiles.size} temporary files...`);
        await Promise.allSettled(
          Array.from(XmlDocxProcessor.tempFiles).map(async (filePath) => {
            try {
              await fs.unlink(filePath);
            } catch (error) {
              // Ignore cleanup errors during shutdown
            }
          })
        );
        XmlDocxProcessor.tempFiles.clear();
      }
    };

    // Handle various exit scenarios
    process.on('exit', cleanup);
    process.on('SIGINT', cleanup);
    process.on('SIGTERM', cleanup);
    process.on('uncaughtException', (error) => {
      console.error('Uncaught exception, cleaning up temp files:', error);
      cleanup().finally(() => process.exit(1));
    });
  }

  /**
   * Process a DOCX document buffer and extract all necessary data
   */
  async processDocumentBuffer(buffer, filename = 'document.docx') {
    const tempFilePath = path.join(require('os').tmpdir(), `temp_${Date.now()}_${Math.random().toString(36).substr(2, 9)}_${filename}`);
    let tempFileCreated = false;

    // Set up cleanup function
    const cleanup = async () => {
      if (tempFileCreated) {
        try {
          await fs.unlink(tempFilePath);
          XmlDocxProcessor.tempFiles.delete(tempFilePath);
          console.log('Temporary file cleaned up:', tempFilePath);
        } catch (error) {
          console.warn('Could not clean up temporary file:', tempFilePath, error.message);
        }
      }
    };

    // Set up timeout cleanup (5 minutes max)
    const timeoutId = setTimeout(async () => {
      console.warn('Temp file cleanup timeout triggered for:', tempFilePath);
      await cleanup();
    }, 5 * 60 * 1000);

    try {
      // Write buffer to temporary file for processing
      await fs.writeFile(tempFilePath, buffer);
      tempFileCreated = true;
      XmlDocxProcessor.tempFiles.add(tempFilePath);

      // Process the temporary file with timeout
      const result = await Promise.race([
        this.processDocument(tempFilePath),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('Document processing timeout')), 2 * 60 * 1000)
        )
      ]);

      return result;
    } finally {
      // Clear timeout and clean up
      clearTimeout(timeoutId);
      await cleanup();
    }
  }

  /**
   * Main processing function using xml2js and PizZip
   */
  async processDocument(filePath) {
    try {
      console.log('Starting XML-based DOCX processing for:', filePath);
      
      const buffer = await fs.readFile(filePath);
      
      // Load DOCX as ZIP archive
      const zip = new PizZip(buffer);
      
      // Extract document structure
      const documentData = await this.extractDocumentXml(zip);
      const stylesData = await this.extractStylesXml(zip);
      const settingsData = await this.extractSettingsXml(zip);
      
      // Extract headers and footers
      const headersFooters = await this.extractHeadersFooters(zip);
      
      // Extract tables with border information
      const tables = await this.extractTablesWithFormatting(zip, documentData);
      
      // Process the extracted XML data
      const textResult = this.extractPlainText(documentData);
      const htmlResult = this.convertToHtml(documentData, stylesData);
      const formattingInfo = this.extractFormattingDetails(documentData, stylesData, settingsData);
      const structure = this.extractDocumentStructure(documentData);
      const styles = this.processStyles(stylesData);
      
      // Add headers/footers and tables to structure
      structure.headersFooters = headersFooters;
      structure.tables = tables;
      
      // Extract italicized text for reference validation
      structure.italicizedText = this.extractItalicizedText(documentData);
      
      console.log('XML-based DOCX processing completed successfully');
      
      return {
        html: htmlResult,
        text: textResult,
        formatting: formattingInfo,
        structure: structure,
        styles: styles,
        headersFooters: headersFooters,
        messages: [{
          type: 'info',
          message: 'Document processed using XML parser for accurate structure extraction'
        }],
        processingInfo: {
          timestamp: new Date().toISOString(),
          fileSize: buffer.length,
          wordCount: textResult.split(/\s+/).filter(Boolean).length,
          processor: 'XmlDocxProcessor'
        }
      };
      
    } catch (error) {
      console.error('Error processing DOCX with XML parser:', error);
      throw new Error(`XML document processing failed: ${error.message}`);
    }
  }

  /**
   * Extract and parse document.xml
   */
  async extractDocumentXml(zip) {
    const docXmlFile = zip.file('word/document.xml');
    if (!docXmlFile) {
      throw new Error('document.xml not found in DOCX file');
    }
    
    const xmlContent = docXmlFile.asText();
    return await this.parser.parseStringPromise(xmlContent);
  }

  /**
   * Extract and parse styles.xml
   */
  async extractStylesXml(zip) {
    const stylesFile = zip.file('word/styles.xml');
    if (!stylesFile) {
      return null;
    }
    
    const xmlContent = stylesFile.asText();
    return await this.parser.parseStringPromise(xmlContent);
  }

  /**
   * Extract and parse settings.xml
   */
  async extractSettingsXml(zip) {
    const settingsFile = zip.file('word/settings.xml');
    if (!settingsFile) {
      return null;
    }
    
    const xmlContent = settingsFile.asText();
    return await this.parser.parseStringPromise(xmlContent);
  }

  /**
   * Extract plain text from document XML
   */
  extractPlainText(documentData) {
    try {
      let text = '';
      const body = documentData['w:document']['w:body'];
      const paragraphs = this.ensureArray(body['w:p']);
      
      paragraphs.forEach((para) => {
        if (!para) return;
        
        const runs = this.ensureArray(para['w:r']);
        runs.forEach(run => {
          if (run && run['w:t']) {
            const textElements = this.ensureArray(run['w:t']);
            textElements.forEach(t => {
              if (typeof t === 'string') {
                text += t;
              } else if (t._ !== undefined) {
                text += t._;
              }
            });
          }
        });
        text += '\n';
      });
      
      return text.trim();
      
    } catch (error) {
      console.error('Error extracting plain text:', error);
      return 'Text extraction failed';
    }
  }

  /**
   * Convert document XML to HTML with proper formatting
   */
  convertToHtml(documentData, stylesData) {
    try {
      let html = '<div class="docx-document">';
      
      const body = documentData['w:document']['w:body'];
      const paragraphs = this.ensureArray(body['w:p']);
      
      paragraphs.forEach((para, index) => {
        if (!para) return;
        
        // Extract paragraph properties
        const pPr = para['w:pPr'] || {};
        const paraStyle = this.extractParagraphStyle(pPr);
        
        // Check if this is a heading
        const isHeading = this.isHeadingParagraph(pPr);
        const tag = isHeading ? this.getHeadingTag(pPr) : 'p';
        
        html += `<${tag} data-para-index="${index}" style="${paraStyle}">`;
        
        // Process runs within paragraph
        const runs = this.ensureArray(para['w:r']);
        runs.forEach((run, runIndex) => {
          if (!run) return;
          
          const runStyle = this.extractRunStyle(run['w:rPr'] || {});
          const runText = this.extractRunText(run);
          
          if (runText) {
            html += `<span data-run-index="${runIndex}" style="${runStyle}">${this.escapeHtml(runText)}</span>`;
          }
        });
        
        html += `</${tag}>`;
      });
      
      html += '</div>';
      return html;
      
    } catch (error) {
      console.error('Error converting to HTML:', error);
      return '<div class="error">HTML conversion failed</div>';
    }
  }

  /**
   * Extract detailed formatting information
   */
  extractFormattingDetails(documentData, stylesData, settingsData) {
    const formatting = {
      document: {
        font: { family: null, size: null },
        spacing: { line: null, paragraph: null },
        margins: { top: null, bottom: null, left: null, right: null },
        indentation: { firstLine: null, hanging: null },
        pageSetup: {}
      },
      paragraphs: [],
      runs: [],
      compliance: {}
    };
    
    try {
      const body = documentData['w:document']['w:body'];
      
      // Extract page setup and margins from sectPr
      const sectPr = body['w:sectPr'];
      if (sectPr) {
        this.extractPageMargins(sectPr, formatting);
        this.extractPageSetup(sectPr, formatting);
      }
      
      // Extract paragraph-level formatting
      const paragraphs = this.ensureArray(body['w:p']);
      
      paragraphs.forEach((para, index) => {
        if (!para) return;
        
        const paraFormatting = this.extractParagraphFormatting(para, index);
        formatting.paragraphs.push(paraFormatting);
      });
      
      // Set document-level defaults
      this.setDocumentDefaults(formatting);
      
      // Calculate APA compliance
      formatting.compliance = this.calculateAPACompliance(formatting);
      
    } catch (error) {
      console.error('Error extracting formatting details:', error);
    }
    
    return formatting;
  }

  /**
   * Extract headers and footers from document
   */
  async extractHeadersFooters(zip) {
    const headersFooters = {
      headers: [],
      footers: [],
      firstPageHeader: null,
      firstPageFooter: null,
      evenPageHeader: null,
      evenPageFooter: null,
      runningHead: null,
      pageNumbers: {
        present: false,
        position: null,
        format: null,
        startNumber: 1
      }
    };
    
    try {
      // Extract header files
      for (let i = 1; i <= 10; i++) {
        const headerFile = zip.file(`word/header${i}.xml`);
        if (headerFile) {
          const headerXml = headerFile.asText();
          const headerData = await this.parser.parseStringPromise(headerXml);
          const headerContent = this.extractHeaderContent(headerData, i);
          
          headersFooters.headers.push(headerContent);
          
          // Check for running head
          if (headerContent.text) {
            const runningHeadMatch = headerContent.text.match(/Running head:\s*(.+)|([A-Z\s]{2,50})/);
            if (runningHeadMatch) {
              headersFooters.runningHead = {
                text: runningHeadMatch[1] || runningHeadMatch[2],
                allCaps: runningHeadMatch[2] === runningHeadMatch[2]?.toUpperCase(),
                length: (runningHeadMatch[1] || runningHeadMatch[2]).length,
                headerIndex: i
              };
            }
          }
        }
        
        // Extract footer files
        const footerFile = zip.file(`word/footer${i}.xml`);
        if (footerFile) {
          const footerXml = footerFile.asText();
          const footerData = await this.parser.parseStringPromise(footerXml);
          const footerContent = this.extractFooterContent(footerData, i);
          
          headersFooters.footers.push(footerContent);
          
          // Check for page numbers
          if (footerContent.hasPageNumber) {
            headersFooters.pageNumbers.present = true;
            headersFooters.pageNumbers.position = footerContent.pageNumberPosition;
          }
        }
      }
      
      // Check document.xml for header/footer references
      const docRelsFile = zip.file('word/_rels/document.xml.rels');
      if (docRelsFile) {
        const relsXml = docRelsFile.asText();
        const relsData = await this.parser.parseStringPromise(relsXml);
        this.mapHeaderFooterTypes(relsData, headersFooters);
      }
      
    } catch (error) {
      console.error('Error extracting headers/footers:', error);
    }
    
    return headersFooters;
  }
  
  /**
   * Extract header content
   */
  extractHeaderContent(headerData, index) {
    const content = {
      index: index,
      text: '',
      hasPageNumber: false,
      pageNumberPosition: null,
      formatting: {}
    };
    
    try {
      const header = headerData['w:hdr'];
      if (header) {
        const paragraphs = this.ensureArray(header['w:p']);
        
        paragraphs.forEach(para => {
          const text = this.extractParagraphText(para);
          content.text += text + ' ';
          
          // Check for page number field
          const runs = this.ensureArray(para['w:r']);
          runs.forEach(run => {
            if (run['w:fldChar'] || run['w:instrText']) {
              const instrText = run['w:instrText'];
              if (instrText && (instrText.includes('PAGE') || instrText.includes('NUMPAGES'))) {
                content.hasPageNumber = true;
                
                // Determine position based on alignment
                const alignment = para['w:pPr']?.['w:jc']?.['$']?.['w:val'];
                content.pageNumberPosition = alignment || 'left';
              }
            }
          });
        });
        
        content.text = content.text.trim();
      }
    } catch (error) {
      console.error('Error extracting header content:', error);
    }
    
    return content;
  }
  
  /**
   * Extract footer content
   */
  extractFooterContent(footerData, index) {
    const content = {
      index: index,
      text: '',
      hasPageNumber: false,
      pageNumberPosition: null,
      formatting: {}
    };
    
    try {
      const footer = footerData['w:ftr'];
      if (footer) {
        const paragraphs = this.ensureArray(footer['w:p']);
        
        paragraphs.forEach(para => {
          const text = this.extractParagraphText(para);
          content.text += text + ' ';
          
          // Check for page number
          const runs = this.ensureArray(para['w:r']);
          runs.forEach(run => {
            if (run['w:fldChar'] || run['w:instrText']) {
              const instrText = run['w:instrText'];
              if (instrText && (instrText.includes('PAGE') || instrText.includes('NUMPAGES'))) {
                content.hasPageNumber = true;
                
                // Determine position
                const alignment = para['w:pPr']?.['w:jc']?.['$']?.['w:val'];
                content.pageNumberPosition = alignment === 'right' ? 'right' : 
                                            alignment === 'center' ? 'center' : 'left';
              }
            }
          });
        });
        
        content.text = content.text.trim();
      }
    } catch (error) {
      console.error('Error extracting footer content:', error);
    }
    
    return content;
  }

  /**
   * Extract document structure (headings, citations, etc.)
   */
  extractDocumentStructure(documentData) {
    const structure = {
      headings: [],
      sections: [],
      citations: [],
      references: [],
      tables: [],
      figures: [],
      italicizedText: []  // Track italicized text for reference validation
    };
    
    try {
      const body = documentData['w:document']['w:body'];
      const paragraphs = this.ensureArray(body['w:p']);
      
      paragraphs.forEach((para, index) => {
        if (!para) return;
        
        const text = this.extractParagraphText(para);
        if (!text.trim()) return;
        
        // Detect headings
        if (this.isHeadingParagraph(para['w:pPr'])) {
          const level = this.getHeadingLevel(para['w:pPr']);
          structure.headings.push({
            text: text.trim(),
            level: level,
            paragraphIndex: index
          });
        }
        
        // Detect citations
        this.extractCitations(text, index, structure.citations);
        
        // Detect special sections
        this.detectSpecialSections(text, index, structure.sections);
        
        // Detect reference entries
        this.detectReferenceEntries(text, index, structure);
      });
      
    } catch (error) {
      console.error('Error extracting document structure:', error);
    }
    
    return structure;
  }

  /**
   * Extract tables with border and formatting information
   */
  async extractTablesWithFormatting(zip, documentData) {
    const tables = [];
    
    try {
      const body = documentData['w:document']['w:body'];
      const tablElements = this.ensureArray(body['w:tbl']);
      
      tablElements.forEach((table, index) => {
        if (!table) return;
        
        const tableInfo = {
          index: index,
          hasVerticalLines: false,
          hasFullBorders: false,
          borderStyle: {},
          cells: [],
          text: ''
        };
        
        // Check table properties for borders
        const tblPr = table['w:tblPr'];
        if (tblPr) {
          const borders = tblPr['w:tblBorders'];
          if (borders) {
            // Check for vertical borders (inside vertical lines)
            if (borders['w:insideV']) {
              const insideV = borders['w:insideV']['$'];
              if (insideV && insideV['w:val'] !== 'nil' && insideV['w:val'] !== 'none') {
                tableInfo.hasVerticalLines = true;
              }
            }
            
            // Check for full borders
            const hasBorder = (borderType) => {
              const border = borders[borderType];
              return border && border['$'] && 
                     border['$']['w:val'] !== 'nil' && 
                     border['$']['w:val'] !== 'none';
            };
            
            tableInfo.hasFullBorders = hasBorder('w:top') && 
                                       hasBorder('w:bottom') && 
                                       hasBorder('w:left') && 
                                       hasBorder('w:right');
            
            // Store border style details
            tableInfo.borderStyle = {
              top: hasBorder('w:top'),
              bottom: hasBorder('w:bottom'),
              left: hasBorder('w:left'),
              right: hasBorder('w:right'),
              insideH: hasBorder('w:insideH'),
              insideV: hasBorder('w:insideV')
            };
          }
        }
        
        // Extract table text content
        const rows = this.ensureArray(table['w:tr']);
        rows.forEach(row => {
          const cells = this.ensureArray(row['w:tc']);
          cells.forEach(cell => {
            const paragraphs = this.ensureArray(cell['w:p']);
            paragraphs.forEach(para => {
              const text = this.extractParagraphText(para);
              tableInfo.text += text + ' ';
            });
          });
        });
        
        tableInfo.text = tableInfo.text.trim();
        tables.push(tableInfo);
      });
      
    } catch (error) {
      console.error('Error extracting tables with formatting:', error);
    }
    
    return tables;
  }
  
  /**
   * Extract italicized text for reference validation
   */
  extractItalicizedText(documentData) {
    const italicizedText = [];
    
    try {
      const body = documentData['w:document']['w:body'];
      const paragraphs = this.ensureArray(body['w:p']);
      
      paragraphs.forEach((para, paraIndex) => {
        if (!para) return;
        
        const runs = this.ensureArray(para['w:r']);
        runs.forEach((run, runIndex) => {
          if (!run) return;
          
          // Check if run has italic formatting
          const rPr = run['w:rPr'];
          if (rPr && rPr['w:i']) {
            const italic = rPr['w:i'];
            // Check if italic is enabled (no val attribute means true, or val="1" or val="true")
            const isItalic = !italic['$'] || 
                           italic['$']['w:val'] === '1' || 
                           italic['$']['w:val'] === 'true' ||
                           italic['$']['w:val'] === 'on';
            
            if (isItalic) {
              const text = this.extractRunText(run);
              if (text) {
                italicizedText.push({
                  text: text,
                  paragraphIndex: paraIndex,
                  runIndex: runIndex,
                  context: this.extractParagraphText(para)
                });
              }
            }
          }
        });
      });
      
    } catch (error) {
      console.error('Error extracting italicized text:', error);
    }
    
    return italicizedText;
  }
  
  /**
   * Map header/footer types from relationships
   */
  mapHeaderFooterTypes(relsData, headersFooters) {
    try {
      const relationships = relsData['Relationships']['Relationship'];
      if (relationships) {
        const rels = this.ensureArray(relationships);
        
        rels.forEach(rel => {
          const type = rel['$']['Type'];
          const target = rel['$']['Target'];
          
          if (type && type.includes('header')) {
            // Determine header type based on target
            if (target.includes('first')) {
              headersFooters.firstPageHeader = target;
            } else if (target.includes('even')) {
              headersFooters.evenPageHeader = target;
            }
          } else if (type && type.includes('footer')) {
            if (target.includes('first')) {
              headersFooters.firstPageFooter = target;
            } else if (target.includes('even')) {
              headersFooters.evenPageFooter = target;
            }
          }
        });
      }
    } catch (error) {
      console.error('Error mapping header/footer types:', error);
    }
  }

  /**
   * Helper methods
   */
  
  ensureArray(item) {
    if (!item) return [];
    return Array.isArray(item) ? item : [item];
  }

  extractParagraphStyle(pPr) {
    let style = '';
    
    // Line spacing
    if (pPr['w:spacing'] && pPr['w:spacing'].$) {
      const spacing = pPr['w:spacing'].$;
      if (spacing['w:line']) {
        const lineHeight = this.lineSpacingToDecimal(spacing['w:line'], spacing['w:lineRule']);
        style += `line-height: ${lineHeight}; `;
      }
    }
    
    // Indentation
    if (pPr['w:ind'] && pPr['w:ind'].$) {
      const ind = pPr['w:ind'].$;
      if (ind['w:firstLine']) {
        const indent = this.twipsToInches(parseInt(ind['w:firstLine']));
        style += `text-indent: ${indent}in; `;
      }
    }
    
    // Alignment
    if (pPr['w:jc'] && pPr['w:jc'].$) {
      const align = pPr['w:jc'].$['w:val'];
      const cssAlign = this.wordAlignToCss(align);
      style += `text-align: ${cssAlign}; `;
    }
    
    return style;
  }

  extractRunStyle(rPr) {
    let style = '';
    
    // Font family
    if (rPr['w:rFonts'] && rPr['w:rFonts'].$) {
      const fontFamily = rPr['w:rFonts'].$['w:ascii'] || rPr['w:rFonts'].$['w:hAnsi'];
      if (fontFamily) {
        style += `font-family: "${fontFamily}", serif; `;
      }
    }
    
    // Font size
    if (rPr['w:sz'] && rPr['w:sz'].$) {
      const fontSize = parseInt(rPr['w:sz'].$['w:val']) / 2;
      style += `font-size: ${fontSize}pt; `;
    }
    
    // Bold
    if (rPr['w:b']) {
      style += 'font-weight: bold; ';
    }
    
    // Italic
    if (rPr['w:i']) {
      style += 'font-style: italic; ';
    }
    
    // Underline
    if (rPr['w:u']) {
      style += 'text-decoration: underline; ';
    }
    
    // Color
    if (rPr['w:color'] && rPr['w:color'].$) {
      const color = rPr['w:color'].$['w:val'];
      if (color && color !== 'auto') {
        style += `color: #${color}; `;
      }
    }
    
    return style;
  }

  extractRunText(run) {
    let text = '';
    
    // Process all child elements in order
    if (run) {
      const children = Object.keys(run);
      children.forEach(key => {
        if (key === 'w:t') {
          // Text content
          const textElements = this.ensureArray(run['w:t']);
          textElements.forEach(t => {
            if (typeof t === 'string') {
              text += t;
            } else if (t._ !== undefined) {
              text += t._;
            }
          });
        } else if (key === 'w:br') {
          // Line breaks - preserve them as actual breaks
          text += '\n';
        } else if (key === 'w:cr') {
          // Carriage returns
          text += '\n';
        } else if (key === 'w:tab') {
          // Tabs
          text += '\t';
        }
      });
    }
    
    return text;
  }

  extractParagraphText(para) {
    let text = '';
    const runs = this.ensureArray(para['w:r']);
    
    runs.forEach(run => {
      text += this.extractRunText(run);
    });
    
    return text;
  }

  isHeadingParagraph(pPr) {
    if (!pPr || !pPr['w:pStyle']) return false;
    
    const styleName = pPr['w:pStyle'].$?.['w:val'] || '';
    return /heading|title/i.test(styleName);
  }

  getHeadingTag(pPr) {
    if (!pPr || !pPr['w:pStyle']) return 'p';
    
    const styleName = pPr['w:pStyle'].$?.['w:val'] || '';
    const match = styleName.match(/heading(\d+)/i);
    
    if (match) {
      const level = Math.min(6, Math.max(1, parseInt(match[1])));
      return `h${level}`;
    }
    
    return styleName.toLowerCase().includes('title') ? 'h1' : 'p';
  }

  getHeadingLevel(pPr) {
    if (!pPr || !pPr['w:pStyle']) return 1;
    
    const styleName = pPr['w:pStyle'].$?.['w:val'] || '';
    const match = styleName.match(/heading(\d+)/i);
    
    return match ? parseInt(match[1]) : 1;
  }

  extractCitations(text, paragraphIndex, citations) {
    const citationPattern = /\(([^)]+),\s*(\d{4})[^)]*\)/g;
    let match;
    
    while ((match = citationPattern.exec(text)) !== null) {
      citations.push({
        text: match[0],
        author: match[1],
        year: match[2],
        paragraphIndex: paragraphIndex,
        position: match.index
      });
    }
  }

  detectSpecialSections(text, index, sections) {
    const textLower = text.toLowerCase().trim();
    
    if (textLower === 'references' || textLower === 'bibliography') {
      sections.push({
        type: 'references',
        startIndex: index,
        title: text
      });
    } else if (textLower === 'abstract') {
      sections.push({
        type: 'abstract',
        startIndex: index,
        title: text
      });
    } else if (textLower.includes('method') && text.length < 50) {
      sections.push({
        type: 'method',
        startIndex: index,
        title: text
      });
    }
  }

  detectReferenceEntries(text, index, structure) {
    const referencesSection = structure.sections.find(s => s.type === 'references');
    if (referencesSection && index > referencesSection.startIndex) {
      if (text.match(/^[A-Z][a-zA-Z-']+,\s+[A-Z].*\(\d{4}\)/)) {
        structure.references.push({
          text: text.trim(),
          paragraphIndex: index,
          type: this.detectReferenceType(text)
        });
      }
    }
  }

  detectReferenceType(text) {
    if (text.match(/\b(?:journal|quarterly|review|proceedings|bulletin)\b/i)) {
      return 'journal';
    }
    if (text.match(/\b(?:publisher|press|books|publication)\b/i)) {
      return 'book';
    }
    if (text.match(/\b(?:http|www\.|\.com|\.org|\.edu|retrieved)\b/i)) {
      return 'website';
    }
    if (text.match(/\b(?:in\s+[A-Z]|\(eds?\.\)|\(ed\.\))\b/i)) {
      return 'chapter';
    }
    return 'unknown';
  }

  // Utility conversion methods
  twipsToInches(twips) {
    return twips / 1440;
  }

  twipsToPoints(twips) {
    return twips / 20;
  }

  lineSpacingToDecimal(value, rule) {
    if (rule === 'auto') {
      return value / 240;
    } else if (rule === 'atLeast' || rule === 'exact') {
      return this.twipsToPoints(value) / 12;
    }
    return 2.0; // Default double spacing
  }

  wordAlignToCss(align) {
    const alignMap = {
      'left': 'left',
      'center': 'center', 
      'right': 'right',
      'both': 'justify',
      'justify': 'justify'
    };
    return alignMap[align] || 'left';
  }

  escapeHtml(text) {
    const div = { innerHTML: '' };
    const textNode = { textContent: text };
    return text
      .replace(/&/g, '&amp;')
      .replace(/</g, '&lt;')
      .replace(/>/g, '&gt;')
      .replace(/"/g, '&quot;')
      .replace(/'/g, '&#39;');
  }

  // Additional helper methods for formatting extraction
  extractPageMargins(sectPr, formatting) {
    const pgMar = sectPr['w:pgMar'];
    if (pgMar && pgMar.$) {
      formatting.document.margins = {
        top: this.twipsToInches(parseInt(pgMar.$['w:top'] || 0)),
        bottom: this.twipsToInches(parseInt(pgMar.$['w:bottom'] || 0)),
        left: this.twipsToInches(parseInt(pgMar.$['w:left'] || 0)),
        right: this.twipsToInches(parseInt(pgMar.$['w:right'] || 0))
      };
    }
  }

  extractPageSetup(sectPr, formatting) {
    const pgSz = sectPr['w:pgSz'];
    if (pgSz && pgSz.$) {
      formatting.document.pageSetup = {
        width: this.twipsToInches(parseInt(pgSz.$['w:w'] || 0)),
        height: this.twipsToInches(parseInt(pgSz.$['w:h'] || 0)),
        orientation: pgSz.$['w:orient'] || 'portrait'
      };
    }
  }

  extractParagraphFormatting(para, index) {
    const paraText = this.extractParagraphText(para);
    const paraFormatting = {
      index,
      text: paraText,
      font: { family: null, size: null, bold: false, italic: false },
      spacing: { line: null, before: null, after: null },
      indentation: { firstLine: null, hanging: null, left: null, right: null },
      alignment: null,
      style: null,
      runs: []
    };

    // Debug paragraph extraction
    if (process.env.NODE_ENV === 'development') {
    }

    // Extract paragraph properties
    const pPr = para['w:pPr'];
    if (pPr) {
      // Style
      if (pPr['w:pStyle'] && pPr['w:pStyle'].$) {
        paraFormatting.style = pPr['w:pStyle'].$['w:val'];
      }

      // Spacing
      if (pPr['w:spacing'] && pPr['w:spacing'].$) {
        const spacing = pPr['w:spacing'].$;
        paraFormatting.spacing = {
          line: spacing['w:line'] ? this.lineSpacingToDecimal(spacing['w:line'], spacing['w:lineRule']) : 2.0,
          before: spacing['w:before'] ? this.twipsToPoints(parseInt(spacing['w:before'])) : null,
          after: spacing['w:after'] ? this.twipsToPoints(parseInt(spacing['w:after'])) : null
        };
      }

      // Indentation
      if (pPr['w:ind'] && pPr['w:ind'].$) {
        const ind = pPr['w:ind'].$;
        paraFormatting.indentation = {
          firstLine: ind['w:firstLine'] ? this.twipsToInches(parseInt(ind['w:firstLine'])) : null,
          hanging: ind['w:hanging'] ? this.twipsToInches(parseInt(ind['w:hanging'])) : null,
          left: ind['w:left'] ? this.twipsToInches(parseInt(ind['w:left'])) : null,
          right: ind['w:right'] ? this.twipsToInches(parseInt(ind['w:right'])) : null
        };
      }

      // Alignment
      if (pPr['w:jc'] && pPr['w:jc'].$) {
        paraFormatting.alignment = pPr['w:jc'].$['w:val'];
      }
    }

    // Extract run-level formatting
    const runs = this.ensureArray(para['w:r']);
    runs.forEach((run, runIndex) => {
      if (!run) return;

      const runFormatting = {
        index: runIndex,
        text: this.extractRunText(run),
        font: { family: null, size: null, bold: false, italic: false, underline: false },
        color: null
      };

      const rPr = run['w:rPr'];
      if (rPr) {
        // Font
        const rFonts = rPr['w:rFonts'];
        if (rFonts && rFonts.$) {
          runFormatting.font.family = rFonts.$['w:ascii'] || rFonts.$['w:hAnsi'] || null;
        }

        // Size - convert from Word's half-points to points
        const sz = rPr['w:sz'];
        if (sz && sz.$) {
          const rawValue = parseInt(sz.$['w:val']);
          runFormatting.font.size = rawValue / 2; // Convert from half-points to points
        }

        // Formatting - check for existence of elements, not their values
        runFormatting.font.bold = 'w:b' in rPr;
        runFormatting.font.italic = 'w:i' in rPr;
        runFormatting.font.underline = 'w:u' in rPr;
        

        // Color
        const color = rPr['w:color'];
        if (color && color.$) {
          runFormatting.color = color.$['w:val'];
        }
      }

      paraFormatting.runs.push(runFormatting);
    });

    // Set paragraph font from first run if available
    if (paraFormatting.runs.length > 0) {
      const firstRun = paraFormatting.runs[0];
      if (firstRun.font.family && !paraFormatting.font.family) {
        paraFormatting.font.family = firstRun.font.family;
      }
      if (firstRun.font.size && !paraFormatting.font.size) {
        paraFormatting.font.size = firstRun.font.size;
      }
    }

    return paraFormatting;
  }

  setDocumentDefaults(formatting) {
    if (formatting.paragraphs.length > 0) {
      const firstPara = formatting.paragraphs.find(p => p.font.family && p.font.size) || formatting.paragraphs[0];
      
      formatting.document.font = { 
        family: firstPara.font.family || null,
        size: firstPara.font.size || null
      };
      
      formatting.document.spacing.line = firstPara.spacing.line || null;
      formatting.document.indentation = { 
        firstLine: firstPara.indentation.firstLine || null,
        hanging: firstPara.indentation.hanging || null 
      };
    }
  }

  processStyles(stylesData) {
    if (!stylesData) {
      return { styles: [], defaultStyle: null };
    }

    const styles = [];
    const stylesRoot = stylesData['w:styles'];
    
    if (stylesRoot && stylesRoot['w:style']) {
      const styleElements = this.ensureArray(stylesRoot['w:style']);
      
      styleElements.forEach(style => {
        if (style && style.$) {
          const styleInfo = {
            id: style.$.styleId,
            name: style['w:name'] ? style['w:name'].$['w:val'] : style.$.styleId,
            type: style.$.type,
            isDefault: style.$.default === '1',
            formatting: {}
          };
          
          // Extract paragraph properties
          const pPr = style['w:pPr'];
          const rPr = style['w:rPr'];
          
          if (pPr) {
            if (pPr['w:spacing'] && pPr['w:spacing'].$) {
              styleInfo.formatting.spacing = pPr['w:spacing'].$;
            }
            if (pPr['w:ind'] && pPr['w:ind'].$) {
              styleInfo.formatting.indentation = pPr['w:ind'].$;
            }
          }
          
          if (rPr) {
            if (rPr['w:rFonts'] && rPr['w:rFonts'].$) {
              styleInfo.formatting.font = rPr['w:rFonts'].$;
            }
            if (rPr['w:sz'] && rPr['w:sz'].$) {
              styleInfo.formatting.fontSize = parseInt(rPr['w:sz'].$['w:val']) / 2;
            }
          }
          
          styles.push(styleInfo);
        }
      });
    }
    
    const defaultStyle = styles.find(s => s.isDefault && s.type === 'paragraph') || 
                        styles.find(s => s.name === 'Normal') ||
                        null;
    
    return { styles, defaultStyle };
  }

  calculateAPACompliance(formatting) {
    const compliance = {
      font: { family: false, size: false, score: 0 },
      spacing: { line: false, score: 0 },
      margins: { all: false, individual: {}, score: 0 },
      indentation: { firstLine: false, score: 0 },
      overall: 0
    };
    
    // Check font compliance
    if (formatting.document.font.family) {
      const fontFamily = formatting.document.font.family.toLowerCase();
      compliance.font.family = fontFamily.includes('times new roman') || 
                              fontFamily.includes('times') ||
                              fontFamily.includes('liberation serif');
    }
    
    if (formatting.document.font.size) {
      compliance.font.size = Math.abs(formatting.document.font.size - 12) < 0.5;
    }
    
    compliance.font.score = (compliance.font.family ? 50 : 0) + (compliance.font.size ? 50 : 0);
    
    // Check spacing compliance
    if (formatting.document.spacing.line) {
      compliance.spacing.line = Math.abs(formatting.document.spacing.line - 2.0) < 0.1;
      compliance.spacing.score = compliance.spacing.line ? 100 : 0;
    }
    
    // Check margins compliance
    if (formatting.document.margins) {
      let marginsCorrect = 0;
      Object.entries(formatting.document.margins).forEach(([side, value]) => {
        const isCorrect = value !== null && Math.abs(value - 1.0) < 0.1;
        compliance.margins.individual[side] = isCorrect;
        if (isCorrect) marginsCorrect++;
      });
      compliance.margins.all = marginsCorrect === 4;
      compliance.margins.score = (marginsCorrect / 4) * 100;
    }
    
    // Check indentation compliance
    const correctIndentation = formatting.paragraphs.filter(p => 
      p.indentation.firstLine !== null && Math.abs(p.indentation.firstLine - 0.5) < 0.05
    ).length;
    
    const totalParagraphs = formatting.paragraphs.filter(p => p.text.trim().length > 0).length;
    
    if (totalParagraphs > 0) {
      compliance.indentation.firstLine = correctIndentation / totalParagraphs > 0.8;
      compliance.indentation.score = (correctIndentation / totalParagraphs) * 100;
    }
    
    // Calculate overall compliance
    const scores = [
      compliance.font.score,
      compliance.spacing.score,
      compliance.margins.score,
      compliance.indentation.score
    ].filter(score => score !== null);
    
    compliance.overall = scores.length > 0 ? scores.reduce((a, b) => a + b) / scores.length : 0;
    
    return compliance;
  }
}

module.exports = XmlDocxProcessor;

// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\routes\docx.js
// server/routes/docx.js - Document processing routes with Worker Thread Pool
const express = require('express');
const multer = require('multer');
const path = require('path');
const fs = require('fs').promises;
const os = require('os');
const WorkerPool = require('../workers/WorkerPool');

// Create router instance
const router = express.Router();

// Initialize Worker Pool for concurrent document processing
// Pool size from environment variable or default to 4 workers
const WORKER_POOL_SIZE = parseInt(process.env.WORKER_POOL_SIZE) || 4;
const workerScript = path.join(__dirname, '../workers/documentProcessor.worker.js');

let workerPool;

// Initialize worker pool only in non-serverless environments
// Vercel and other serverless platforms don't support worker threads well
if (!process.env.VERCEL) {
  try {
    workerPool = new WorkerPool(WORKER_POOL_SIZE, workerScript);
    console.log(`‚úÖ Worker Pool initialized with ${WORKER_POOL_SIZE} workers`);

    // Graceful shutdown handler
    const shutdownHandler = async () => {
      console.log('Shutting down Worker Pool...');
      if (workerPool) {
        await workerPool.shutdown();
      }
    };

    process.on('SIGTERM', shutdownHandler);
    process.on('SIGINT', shutdownHandler);
  } catch (error) {
    console.error('‚ùå Failed to initialize Worker Pool:', error);
    console.error('‚ö†Ô∏è Falling back to direct processing (no concurrency)');
    workerPool = null;
  }
} else {
  console.log('‚ö†Ô∏è Serverless environment detected - Worker Pool disabled');
  workerPool = null;
}

// Fallback processors for when Worker Pool is not available
const XmlDocxProcessor = require('../processors/XmlDocxProcessor');
const DocxModifier = require('../processors/DocxModifier');
const xmlDocxProcessor = new XmlDocxProcessor();
const docxModifier = new DocxModifier();

// Configure multer for file uploads
const storage = process.env.VERCEL
  ? multer.memoryStorage() // Memory storage for Vercel serverless
  : multer.diskStorage({   // Disk storage for local development
      destination: (req, file, cb) => {
        cb(null, path.join(__dirname, '../uploads/'));
      },
      filename: (req, file, cb) => {
        const uniqueSuffix = Date.now() + '-' + Math.round(Math.random() * 1E9);
        const extension = path.extname(file.originalname);
        cb(null, `document-${uniqueSuffix}${extension}`);
      }
    });

// File filter to only allow DOCX files
const fileFilter = (req, file, cb) => {
  const allowedMimes = [
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    'application/octet-stream' // Some systems send DOCX as octet-stream
  ];

  const allowedExtensions = ['.docx'];
  const fileExtension = path.extname(file.originalname).toLowerCase();

  if (allowedMimes.includes(file.mimetype) || allowedExtensions.includes(fileExtension)) {
    cb(null, true);
  } else {
    cb(new Error('Only DOCX files are allowed'), false);
  }
};

const upload = multer({
  storage: storage,
  fileFilter: fileFilter,
  limits: {
    fileSize: 10 * 1024 * 1024, // 10MB limit
    files: 1
  }
});

/**
 * Validate DOCX file by checking ZIP signature
 */
function isValidDocxFile(buffer) {
  if (!buffer || buffer.length < 4) {
    return false;
  }

  // DOCX files are ZIP archives starting with PK signature (0x50 0x4B 0x03 0x04)
  return buffer[0] === 0x50 && buffer[1] === 0x4B &&
         buffer[2] === 0x03 && buffer[3] === 0x04;
}

/**
 * POST /api/upload-docx
 * Upload and process a DOCX file using Worker Pool for concurrent processing
 */
router.post('/upload-docx', upload.single('document'), async (req, res) => {
  const startTime = Date.now();
  let filePath = null;

  try {
    // Validate file upload
    if (!req.file) {
      return res.status(400).json({
        success: false,
        error: 'No file uploaded. Please select a DOCX file.',
        code: 'NO_FILE'
      });
    }

    console.log(`üì• Processing uploaded file: ${req.file.originalname} (${req.file.size} bytes)`);

    let fileBuffer;

    // Get file buffer (from memory or disk)
    if (req.file.buffer) {
      // Memory storage (Vercel or explicit memory storage)
      fileBuffer = req.file.buffer;
    } else if (req.file.path) {
      // Disk storage (local development)
      filePath = req.file.path;
      fileBuffer = await fs.readFile(filePath);
    } else {
      throw new Error('Unable to read uploaded file');
    }

    // Validate DOCX file
    if (!isValidDocxFile(fileBuffer)) {
      throw new Error('File is not a valid DOCX document');
    }

    // Process document using Worker Pool if available, otherwise use direct processing
    let result;
    let processingMethod;

    if (workerPool) {
      // ‚úÖ Worker Pool available - process concurrently
      console.log(`üîÑ Sending job to Worker Pool (available workers: ${workerPool.getStats().availableWorkers})`);
      processingMethod = 'worker-pool';

      try {
        const workerResult = await workerPool.executeJob({
          type: 'upload',
          data: {
            buffer: fileBuffer,
            filename: req.file.originalname
          }
        }, 60000); // 60 second timeout

        result = workerResult.document;

        console.log(`‚úÖ Worker Pool processing completed`);
        console.log(`üìä Pool stats:`, workerPool.getStats());

      } catch (error) {
        console.error('‚ùå Worker Pool processing failed:', error.message);

        // If worker pool fails, fall back to direct processing
        console.log('‚ö†Ô∏è Falling back to direct processing');
        processingMethod = 'direct-fallback';
        result = await xmlDocxProcessor.processDocumentBuffer(fileBuffer, req.file.originalname);
      }

    } else {
      // ‚ö†Ô∏è Worker Pool not available - use direct processing
      console.log('üìÑ Processing directly (no Worker Pool)');
      processingMethod = 'direct';
      result = await xmlDocxProcessor.processDocumentBuffer(fileBuffer, req.file.originalname);
    }

    const processingTime = Date.now() - startTime;
    console.log(`‚úÖ Document processed successfully in ${processingTime}ms (method: ${processingMethod})`);

    // Add processing metadata
    result.processingInfo = {
      ...result.processingInfo,
      processingTime: processingTime,
      originalFilename: req.file.originalname,
      fileSize: req.file.size,
      processingMethod: processingMethod,
      workerPoolEnabled: !!workerPool,
      serverless: !!process.env.VERCEL,
      platform: process.env.VERCEL ? 'vercel' : 'traditional'
    };

    // Validate processing results
    if (!result.text || !result.html) {
      throw new Error('Document processing produced incomplete results');
    }

    // Clean up uploaded file if it exists (disk storage)
    if (filePath) {
      try {
        await fs.unlink(filePath);
        console.log('üóëÔ∏è Cleaned up temporary file');
      } catch (cleanupError) {
        console.warn('‚ö†Ô∏è Failed to clean up temporary file:', cleanupError.message);
      }
      filePath = null;
    }

    // Return success response
    res.json({
      success: true,
      document: result,
      message: 'Document processed successfully'
    });

  } catch (error) {
    console.error('‚ùå Error processing document:', error);

    // Clean up uploaded file if it exists
    if (filePath) {
      try {
        await fs.unlink(filePath);
      } catch (cleanupError) {
        console.error('Error cleaning up file:', cleanupError);
      }
    }

    // Determine error type and appropriate response
    let statusCode = 500;
    let errorCode = 'PROCESSING_ERROR';
    let errorMessage = 'Failed to process document';

    if (error.message.includes('not a valid DOCX')) {
      statusCode = 400;
      errorCode = 'INVALID_FILE';
      errorMessage = 'File is not a valid DOCX document';
    } else if (error.message.includes('DOCX processing failed')) {
      statusCode = 422;
      errorCode = 'PROCESSING_FAILED';
      errorMessage = 'Document could not be processed';
    } else if (error.message.includes('not readable')) {
      statusCode = 400;
      errorCode = 'FILE_UNREADABLE';
      errorMessage = 'Uploaded file could not be read';
    } else if (error.message.includes('timeout')) {
      statusCode = 504;
      errorCode = 'PROCESSING_TIMEOUT';
      errorMessage = 'Document processing timed out';
    }

    res.status(statusCode).json({
      success: false,
      error: errorMessage,
      code: errorCode,
      details: process.env.NODE_ENV === 'development' ? error.message : undefined
    });
  }
});

/**
 * POST /api/apply-fix
 * Apply a formatting fix to a DOCX document using Worker Pool
 */
router.post('/apply-fix', async (req, res) => {
  console.log('üéØ /api/apply-fix endpoint called');

  // Memory monitoring
  const initialMemory = process.memoryUsage();
  console.log(`üß† Initial memory usage: ${Math.round(initialMemory.heapUsed / 1024 / 1024)}MB`);

  // Request size validation
  const contentLength = req.get('content-length');
  if (contentLength && parseInt(contentLength) > 50 * 1024 * 1024) { // 50MB limit
    console.warn(`‚ö†Ô∏è Request too large: ${Math.round(parseInt(contentLength) / 1024 / 1024)}MB`);
    return res.status(413).json({
      success: false,
      error: 'Request too large. Maximum size is 50MB.',
      code: 'REQUEST_TOO_LARGE'
    });
  }

  try {
    // Parse request data
    let requestData;

    if (req.is('multipart/form-data')) {
      console.log('üì¶ Processing multipart form data');
      requestData = {
        fixAction: req.body.fixAction,
        fixValue: req.body.fixValue,
        originalFilename: req.body.originalFilename,
        documentBuffer: req.body.documentBuffer
      };
    } else {
      console.log('üì¶ Processing JSON data');
      requestData = req.body;
    }

    const { documentBuffer, fixAction, fixValue, originalFilename } = requestData;

    // Validate input
    if (!documentBuffer || !fixAction) {
      return res.status(400).json({
        success: false,
        error: 'Missing required parameters: documentBuffer and fixAction',
        code: 'MISSING_PARAMS'
      });
    }

    // Check if the fix is supported
    const supportedFixes = [
      'fixFont', 'fixFontSize', 'fixLineSpacing', 'fixMargins', 'fixIndentation',
      'addCitationComma', 'fixParentheticalConnector', 'fixEtAlFormatting',
      'fixReferenceConnector', 'fixAllCapsHeading', 'addPageNumber'
    ];

    if (!supportedFixes.includes(fixAction)) {
      return res.status(400).json({
        success: false,
        error: `Unsupported fix action: ${fixAction}`,
        code: 'UNSUPPORTED_FIX'
      });
    }

    console.log(`üîß Applying fix: ${fixAction}`);
    console.log(`üìã Fix value:`, fixValue);

    // Convert base64 to buffer if needed
    let docxBuffer;
    if (typeof documentBuffer === 'string') {
      try {
        docxBuffer = Buffer.from(documentBuffer, 'base64');
        console.log(`‚úÖ Buffer conversion successful, size: ${docxBuffer.length} bytes`);

        // Validate converted buffer size
        if (docxBuffer.length > 50 * 1024 * 1024) { // 50MB limit
          return res.status(413).json({
            success: false,
            error: 'Document buffer too large. Maximum file size is 50MB.',
            code: 'BUFFER_TOO_LARGE'
          });
        }
      } catch (error) {
        return res.status(400).json({
          success: false,
          error: `Invalid base64 document buffer: ${error.message}`,
          code: 'INVALID_BUFFER'
        });
      }
    } else {
      docxBuffer = Buffer.from(documentBuffer);

      // Validate buffer size
      if (docxBuffer.length > 50 * 1024 * 1024) {
        return res.status(413).json({
          success: false,
          error: 'Document buffer too large. Maximum file size is 50MB.',
          code: 'BUFFER_TOO_LARGE'
        });
      }
    }

    // Process fix using Worker Pool if available
    let modificationResult;
    let reprocessingResult;
    let processingMethod;

    if (workerPool) {
      // ‚úÖ Worker Pool available - process concurrently
      console.log(`üîÑ Sending fix job to Worker Pool`);
      processingMethod = 'worker-pool';

      try {
        // Step 1: Apply fix via worker
        const fixResult = await workerPool.executeJob({
          type: 'fix',
          data: {
            buffer: docxBuffer,
            fixAction: fixAction,
            fixValue: fixValue,
            filename: originalFilename || 'document.docx'
          }
        }, 60000); // 60 second timeout

        const modifiedBuffer = fixResult.modifiedBuffer;
        console.log(`‚úÖ Fix applied successfully, reprocessing document...`);

        // Step 2: Reprocess the modified document via worker
        const reprocessResult = await workerPool.executeJob({
          type: 'upload',
          data: {
            buffer: modifiedBuffer,
            filename: originalFilename || 'document.docx'
          }
        }, 60000);

        reprocessingResult = reprocessResult.document;
        modificationResult = { success: true, buffer: modifiedBuffer };

        console.log(`‚úÖ Worker Pool fix processing completed`);

      } catch (error) {
        console.error('‚ùå Worker Pool fix processing failed:', error.message);

        // Fall back to direct processing
        console.log('‚ö†Ô∏è Falling back to direct processing');
        processingMethod = 'direct-fallback';

        modificationResult = await docxModifier.applyFormattingFix(docxBuffer, fixAction, fixValue);

        if (!modificationResult.success) {
          return res.status(500).json({
            success: false,
            error: `Failed to apply fix: ${modificationResult.error}`,
            code: 'FIX_APPLICATION_FAILED'
          });
        }

        reprocessingResult = await xmlDocxProcessor.processDocumentBuffer(
          modificationResult.buffer,
          originalFilename || 'document.docx'
        );
      }

    } else {
      // ‚ö†Ô∏è Worker Pool not available - use direct processing
      console.log('üîß Processing fix directly (no Worker Pool)');
      processingMethod = 'direct';

      // Apply fix
      modificationResult = await docxModifier.applyFormattingFix(docxBuffer, fixAction, fixValue);

      if (!modificationResult.success) {
        return res.status(500).json({
          success: false,
          error: `Failed to apply fix: ${modificationResult.error}`,
          code: 'FIX_APPLICATION_FAILED'
        });
      }

      // Reprocess document
      reprocessingResult = await xmlDocxProcessor.processDocumentBuffer(
        modificationResult.buffer,
        originalFilename || 'document.docx'
      );
    }

    // Monitor memory usage
    const finalMemory = process.memoryUsage();
    const memoryUsed = Math.round((finalMemory.heapUsed - initialMemory.heapUsed) / 1024 / 1024);
    console.log(`üß† Final memory: ${Math.round(finalMemory.heapUsed / 1024 / 1024)}MB (+${memoryUsed}MB)`);

    // Add processing metadata
    reprocessingResult.processingInfo = {
      ...reprocessingResult.processingInfo,
      originalFilename: originalFilename || 'unknown.docx',
      fixApplied: fixAction,
      fixValue: fixValue,
      processingMethod: processingMethod,
      workerPoolEnabled: !!workerPool,
      memoryUsage: {
        initial: Math.round(initialMemory.heapUsed / 1024 / 1024),
        peak: Math.round(finalMemory.heapUsed / 1024 / 1024),
        delta: memoryUsed
      }
    };

    // Ensure buffer is properly converted to base64 string
    const bufferToConvert = Buffer.isBuffer(modificationResult.buffer)
      ? modificationResult.buffer
      : Buffer.from(modificationResult.buffer);

    const base64String = bufferToConvert.toString('base64');
    console.log(`üì¶ Buffer converted to base64, length: ${base64String.length}`);

    // Return the reprocessed document with the modified buffer
    res.json({
      success: true,
      document: reprocessingResult,
      modifiedDocumentBuffer: base64String,
      fixApplied: fixAction,
      message: `Successfully applied ${fixAction} and reprocessed document`
    });

    // Force garbage collection if available
    if (global.gc) {
      console.log('üóëÔ∏è Running garbage collection...');
      global.gc();
    }

  } catch (error) {
    console.error('‚ùå Critical error in apply-fix route:', error);

    // Monitor memory on error
    const errorMemory = process.memoryUsage();
    console.log(`üß† Error memory usage: ${Math.round(errorMemory.heapUsed / 1024 / 1024)}MB`);

    // Force garbage collection on error
    if (global.gc) {
      console.log('üóëÔ∏è Running garbage collection after error...');
      global.gc();
    }

    // Handle specific error types
    let statusCode = 500;
    let errorMessage = 'Failed to apply fix to document';
    let errorCode = 'APPLY_FIX_ERROR';

    if (error.message.includes('timed out') || error.message.includes('timeout')) {
      statusCode = 504;
      errorMessage = 'Request timed out while processing document';
      errorCode = 'PROCESSING_TIMEOUT';
    } else if (error.message.includes('too large') || error.message.includes('ENOMEM')) {
      statusCode = 413;
      errorMessage = 'Document too large to process';
      errorCode = 'DOCUMENT_TOO_LARGE';
    }

    res.status(statusCode).json({
      success: false,
      error: errorMessage,
      code: errorCode,
      details: process.env.NODE_ENV === 'development' ? error.message : undefined
    });
  }
});

/**
 * POST /api/process-document
 * Process a document from Supabase Storage
 * Triggered after user uploads to Supabase
 */
router.post('/process-document', async (req, res) => {
  console.log('üì• Processing document from Supabase Storage');

  try {
    const { documentId } = req.body;

    if (!documentId) {
      return res.status(400).json({
        success: false,
        error: 'Missing documentId',
        code: 'MISSING_PARAMETERS'
      });
    }

    // Import Supabase client
    const supabase = require('../utils/supabaseClient');

    // Extract and verify JWT token from Authorization header
    const authHeader = req.headers.authorization;
    if (!authHeader || !authHeader.startsWith('Bearer ')) {
      return res.status(401).json({
        success: false,
        error: 'Missing or invalid authorization header',
        code: 'UNAUTHORIZED'
      });
    }

    const token = authHeader.replace('Bearer ', '');

    // Verify token and get authenticated user
    const { data: { user }, error: authError } = await supabase.auth.getUser(token);

    if (authError || !user) {
      return res.status(401).json({
        success: false,
        error: 'Invalid or expired token',
        code: 'UNAUTHORIZED'
      });
    }

    console.log(`üîê Authenticated user: ${user.id}`);

    // Fetch document metadata from database
    // Now using verified user.id instead of trusting request body
    const { data: document, error: fetchError } = await supabase
      .from('documents')
      .select('*')
      .eq('id', documentId)
      .eq('user_id', user.id)
      .single();

    if (fetchError || !document) {
      return res.status(404).json({
        success: false,
        error: 'Document not found',
        code: 'DOCUMENT_NOT_FOUND'
      });
    }

    // Update status to processing
    await supabase
      .from('documents')
      .update({ status: 'processing' })
      .eq('id', documentId);

    // Download file from Supabase Storage
    const { data: fileData, error: downloadError } = await supabase.storage
      .from('user-documents')
      .download(document.file_path);

    if (downloadError || !fileData) {
      await supabase
        .from('documents')
        .update({ status: 'failed' })
        .eq('id', documentId);

      return res.status(500).json({
        success: false,
        error: 'Failed to download document from storage',
        code: 'DOWNLOAD_ERROR'
      });
    }

    // Convert Blob to Buffer
    const arrayBuffer = await fileData.arrayBuffer();
    const fileBuffer = Buffer.from(arrayBuffer);

    // Validate DOCX file
    if (!isValidDocxFile(fileBuffer)) {
      await supabase
        .from('documents')
        .update({ status: 'failed' })
        .eq('id', documentId);

      return res.status(400).json({
        success: false,
        error: 'Invalid DOCX file format',
        code: 'INVALID_DOCX'
      });
    }

    // Process document via Worker Pool or direct processing
    let result;
    let processingMethod;

    if (workerPool) {
      console.log(`üîÑ Sending job to Worker Pool for document ${documentId}`);
      processingMethod = 'worker-pool';

      try {
        const workerResult = await workerPool.executeJob({
          type: 'upload',
          data: { buffer: fileBuffer, filename: document.filename }
        }, 60000);
        result = workerResult.document;
      } catch (error) {
        console.log('‚ö†Ô∏è Falling back to direct processing');
        processingMethod = 'direct-fallback';
        result = await xmlDocxProcessor.processDocumentBuffer(fileBuffer, document.filename);
      }
    } else {
      console.log('üìÑ Processing directly (Worker Pool not available)');
      processingMethod = 'direct';
      result = await xmlDocxProcessor.processDocumentBuffer(fileBuffer, document.filename);
    }

    // Store document data without analysis - frontend will run full analysis when loaded
    console.log('üì¶ Storing document data (analysis will run on frontend)');
    const issues = [];
    const complianceScore = null;
    const issueCount = 0;

    // Store analysis results
    const { error: insertError } = await supabase
      .from('analysis_results')
      .insert({
        document_id: documentId,
        compliance_score: complianceScore,
        issue_count: issueCount,
        issues: issues,
        document_data: result
      });

    if (insertError) {
      console.error('Failed to save analysis results:', insertError);
    }

    // Update document status to completed
    await supabase
      .from('documents')
      .update({
        status: 'completed',
        processed_at: new Date().toISOString()
      })
      .eq('id', documentId);

    console.log(`‚úÖ Document ${documentId} processed successfully`);

    res.json({
      success: true,
      documentId,
      processingMethod,
      complianceScore,
      issueCount
    });

  } catch (error) {
    console.error('‚ùå Error processing document:', error);

    // Try to update document status to failed
    try {
      const { documentId } = req.body;
      if (documentId) {
        const supabase = require('../utils/supabaseClient');
        await supabase
          .from('documents')
          .update({ status: 'failed' })
          .eq('id', documentId);
      }
    } catch (updateError) {
      console.error('Failed to update document status:', updateError);
    }

    res.status(500).json({
      success: false,
      error: 'Failed to process document',
      code: 'PROCESSING_ERROR',
      details: process.env.NODE_ENV === 'development' ? error.message : undefined
    });
  }
});

/**
 * GET /api/worker-stats
 * Get Worker Pool statistics (for monitoring/debugging)
 */
router.get('/worker-stats', (req, res) => {
  if (!workerPool) {
    return res.json({
      enabled: false,
      message: 'Worker Pool is not enabled (serverless environment or initialization failed)'
    });
  }

  const stats = workerPool.getStats();
  res.json({
    enabled: true,
    stats: stats
  });
});

// Error handling middleware for multer errors
router.use((error, req, res, next) => {
  // Handle multer errors
  if (error.code === 'LIMIT_FILE_SIZE') {
    return res.status(413).json({
      success: false,
      error: 'File too large. Maximum size is 10MB.',
      code: 'FILE_TOO_LARGE'
    });
  }

  if (error.code === 'LIMIT_UNEXPECTED_FILE') {
    return res.status(400).json({
      success: false,
      error: 'Unexpected file. Please upload only one DOCX file.',
      code: 'UNEXPECTED_FILE'
    });
  }

  if (error.message.includes('multer')) {
    return res.status(400).json({
      success: false,
      error: error.message,
      code: 'UPLOAD_ERROR'
    });
  }

  // Handle file filter errors
  if (error.message === 'Only DOCX files are allowed') {
    return res.status(400).json({
      success: false,
      error: 'Only DOCX files are allowed',
      code: 'INVALID_FILE_TYPE'
    });
  }

  // Pass other errors to main error handler
  next(error);
});

/**
 * POST /api/save-edits - DEPRECATED ENDPOINT
 * This endpoint is deprecated and will be removed in a future version.
 *
 * NEW ARCHITECTURE:
 * - Manual edits are now saved as Tiptap JSON directly to Supabase (client-side)
 * - No server-side DOCX manipulation for edits
 * - DOCX is generated fresh on export only (see /api/export-docx)
 *
 * This stub returns an error to guide developers to the new approach.
 */
router.post('/save-edits', async (req, res, next) => {
  // ENDPOINT DEPRECATED - Return error with migration guidance
  console.warn('‚ö†Ô∏è DEPRECATED: /api/save-edits endpoint called');

  res.status(410).json({
    success: false,
    error: 'This endpoint is deprecated.',
    deprecated: true,
    message: 'Manual edits are now saved as Tiptap JSON directly to Supabase (client-side). This endpoint is no longer needed.',
    migration: {
      oldFlow: 'Client ‚Üí /api/save-edits ‚Üí DOCX manipulation ‚Üí Supabase',
      newFlow: 'Client ‚Üí Supabase (tiptap_content column)',
      clientMethod: 'documentService.autoSaveDocument() - saves JSON directly',
      exportMethod: 'Use /api/export-docx to generate DOCX from JSON when needed'
    },
    code: 'ENDPOINT_DEPRECATED'
  });
});

// Export the router and worker pool for health check access
module.exports = router;
module.exports.workerPool = workerPool;


// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\utils\supabaseClient.js
// server/utils/supabaseClient.js - Express backend Supabase client
const { createClient } = require('@supabase/supabase-js');

/**
 * Supabase client for Express backend
 * Uses service role key for admin-level operations
 *
 * IMPORTANT: This client bypasses Row Level Security (RLS)
 * Use with caution and validate user permissions manually
 *
 * Environment variables should be in root .env file:
 * - NEXT_PUBLIC_SUPABASE_URL (same URL used by frontend)
 * - SUPABASE_SERVICE_ROLE_KEY (backend only, never expose to browser)
 */
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false
    }
  }
);

module.exports = supabase;


// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\workers\documentProcessor.worker.js
// server/workers/documentProcessor.worker.js - Worker thread for document processing
const { parentPort } = require('worker_threads');
const XmlDocxProcessor = require('../processors/XmlDocxProcessor');
const DocxModifier = require('../processors/DocxModifier');

// Initialize processors
const xmlProcessor = new XmlDocxProcessor();
const docxModifier = new DocxModifier();

// Track processing for logging
let jobsProcessed = 0;

console.log(`üîß Document processor worker initialized (PID: ${process.pid})`);

/**
 * Listen for messages from main thread
 */
parentPort.on('message', async (message) => {
  const { jobId, type, data } = message;
  const startTime = Date.now();

  console.log(`üì• Worker received job ${jobId} (type: ${type})`);

  try {
    let result;

    switch (type) {
      case 'upload':
        result = await processUpload(data);
        break;

      case 'fix':
        result = await processFix(data);
        break;

      case 'save-edits':
        result = await processSaveEdits(data);
        break;

      default:
        throw new Error(`Unknown job type: ${type}`);
    }

    const processingTime = Date.now() - startTime;
    jobsProcessed++;

    console.log(`‚úÖ Worker completed job ${jobId} in ${processingTime}ms (total processed: ${jobsProcessed})`);

    // Send success response back to main thread
    parentPort.postMessage({
      jobId,
      success: true,
      result,
      processingTime
    });

  } catch (error) {
    const processingTime = Date.now() - startTime;

    console.error(`‚ùå Worker failed job ${jobId} after ${processingTime}ms:`, error.message);

    // Send error response back to main thread
    parentPort.postMessage({
      jobId,
      success: false,
      error: error.message,
      processingTime
    });
  }
});

/**
 * Process document upload (XML parsing and APA analysis data extraction)
 */
async function processUpload(data) {
  const { buffer, filename } = data;

  console.log(`üìÑ Processing upload: ${filename} (${buffer.length} bytes)`);

  // Validate input
  if (!buffer || buffer.length === 0) {
    throw new Error('Invalid or empty document buffer');
  }

  if (!filename) {
    throw new Error('Filename is required');
  }

  // Process document buffer using XmlDocxProcessor
  const result = await xmlProcessor.processDocumentBuffer(buffer, filename);

  // Validate result
  if (!result || !result.text || !result.html) {
    throw new Error('Document processing returned incomplete data');
  }

  console.log(`‚úÖ Upload processed: ${result.text.length} chars, ${result.formatting?.paragraphs?.length || 0} paragraphs`);

  return {
    document: result,
    stats: {
      textLength: result.text.length,
      htmlLength: result.html.length,
      paragraphCount: result.formatting?.paragraphs?.length || 0,
      wordCount: result.processingInfo?.wordCount || 0
    }
  };
}

/**
 * Process formatting fix application (DOCX modification)
 */
async function processFix(data) {
  const { buffer, fixAction, fixValue, filename } = data;

  console.log(`üîß Processing fix: ${fixAction} for ${filename}`);

  // Validate input
  if (!buffer || buffer.length === 0) {
    throw new Error('Invalid or empty document buffer');
  }

  if (!fixAction) {
    throw new Error('Fix action is required');
  }

  // Apply fix using DocxModifier
  const fixResult = await docxModifier.applyFormattingFix(buffer, fixAction, fixValue);

  if (!fixResult.success) {
    throw new Error(fixResult.error || 'Fix application failed');
  }

  console.log(`‚úÖ Fix applied: ${fixAction}`);

  return {
    modifiedBuffer: fixResult.buffer,
    fixAction,
    success: true
  };
}

/**
 * Process manual edit saves (text changes to DOCX)
 */
async function processSaveEdits(data) {
  const { buffer, paragraphs, filename } = data;

  console.log(`üíæ Processing save-edits: ${paragraphs.length} paragraphs for ${filename}`);

  // Validate input
  if (!buffer || buffer.length === 0) {
    throw new Error('Invalid or empty document buffer');
  }

  if (!paragraphs || paragraphs.length === 0) {
    throw new Error('Paragraphs array is required');
  }

  // Apply text changes using DocxModifier
  const saveResult = await docxModifier.applyTextChanges(buffer, paragraphs);

  if (!saveResult.success) {
    throw new Error(saveResult.error || 'Save edits failed');
  }

  console.log(`‚úÖ Text changes applied: ${saveResult.changesApplied} paragraphs updated`);

  return {
    modifiedBuffer: saveResult.buffer,
    changesApplied: saveResult.changesApplied,
    success: true
  };
}

/**
 * Handle worker errors
 */
process.on('uncaughtException', (error) => {
  console.error('‚ùå Uncaught exception in worker:', error);
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('‚ùå Unhandled rejection in worker:', reason);
  process.exit(1);
});

console.log(`‚úÖ Document processor worker ready and listening for jobs`);


// File: c:\Users\Taimoor\Documents\GitHub\apa-document-checker\server\workers\WorkerPool.js
// server/workers/WorkerPool.js - Worker Thread Pool Manager for concurrent document processing
const { Worker } = require('worker_threads');
const path = require('path');
const { EventEmitter } = require('events');

/**
 * WorkerPool manages a pool of worker threads for concurrent document processing
 * Ensures multiple users can process documents simultaneously without blocking
 */
class WorkerPool extends EventEmitter {
  constructor(poolSize = 4, workerScript) {
    super();

    this.poolSize = poolSize;
    this.workerScript = workerScript;
    this.workers = [];
    this.availableWorkers = [];
    this.busyWorkers = new Map(); // worker -> jobId
    this.jobQueue = [];
    this.activeJobs = new Map(); // jobId -> { resolve, reject, timeout, jobData }
    this.isShuttingDown = false;

    // Statistics
    this.stats = {
      totalJobsProcessed: 0,
      totalJobsFailed: 0,
      currentQueueSize: 0,
      peakQueueSize: 0
    };

    // Initialize worker pool
    this._initializeWorkers();

    console.log(`‚úÖ WorkerPool initialized with ${poolSize} workers`);
  }

  /**
   * Initialize all worker threads
   */
  _initializeWorkers() {
    for (let i = 0; i < this.poolSize; i++) {
      this._createWorker(i);
    }
  }

  /**
   * Create a single worker thread
   */
  _createWorker(workerId) {
    const worker = new Worker(this.workerScript);
    worker.workerId = workerId;
    worker.isAvailable = true;
    worker.currentJobId = null;

    // Handle messages from worker
    worker.on('message', (message) => {
      this._handleWorkerMessage(worker, message);
    });

    // Handle worker errors
    worker.on('error', (error) => {
      this._handleWorkerError(worker, error);
    });

    // Handle worker exit
    worker.on('exit', (code) => {
      this._handleWorkerExit(worker, code);
    });

    this.workers.push(worker);
    this.availableWorkers.push(worker);

    console.log(`Worker ${workerId} created and ready`);
  }

  /**
   * Execute a job on an available worker
   * Returns a promise that resolves with the job result
   */
  executeJob(jobData, timeout = 60000) {
    if (this.isShuttingDown) {
      return Promise.reject(new Error('WorkerPool is shutting down'));
    }

    // Generate unique job ID
    const jobId = `job_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    jobData.jobId = jobId;

    console.log(`üì• Job ${jobId} received (type: ${jobData.type})`);

    return new Promise((resolve, reject) => {
      // Set up timeout
      const timeoutId = setTimeout(() => {
        this._handleJobTimeout(jobId);
      }, timeout);

      // Store job metadata
      this.activeJobs.set(jobId, {
        resolve,
        reject,
        timeout: timeoutId,
        jobData,
        startTime: Date.now()
      });

      // Try to assign to an available worker
      const worker = this._getAvailableWorker();
      if (worker) {
        this._assignJobToWorker(worker, jobData);
      } else {
        // No available worker, add to queue
        this.jobQueue.push(jobData);
        this.stats.currentQueueSize = this.jobQueue.length;
        this.stats.peakQueueSize = Math.max(this.stats.peakQueueSize, this.stats.currentQueueSize);
        console.log(`‚è≥ Job ${jobId} queued (queue size: ${this.jobQueue.length})`);
      }
    });
  }

  /**
   * Get an available worker from the pool
   */
  _getAvailableWorker() {
    return this.availableWorkers.shift() || null;
  }

  /**
   * Assign a job to a specific worker
   */
  _assignJobToWorker(worker, jobData) {
    worker.isAvailable = false;
    worker.currentJobId = jobData.jobId;
    this.busyWorkers.set(worker, jobData.jobId);

    console.log(`üîÑ Assigning job ${jobData.jobId} to worker ${worker.workerId}`);

    // Send job to worker
    worker.postMessage(jobData);
  }

  /**
   * Handle message received from worker
   */
  _handleWorkerMessage(worker, message) {
    const { jobId, success, result, error, processingTime } = message;

    console.log(`üì§ Worker ${worker.workerId} completed job ${jobId} (success: ${success}, time: ${processingTime}ms)`);

    // Get job metadata
    const job = this.activeJobs.get(jobId);
    if (!job) {
      console.warn(`‚ö†Ô∏è Received result for unknown job ${jobId}`);
      return;
    }

    // Clear timeout
    clearTimeout(job.timeout);

    // Update statistics
    if (success) {
      this.stats.totalJobsProcessed++;
      job.resolve(result);
    } else {
      this.stats.totalJobsFailed++;
      job.reject(new Error(error || 'Worker processing failed'));
    }

    // Clean up
    this.activeJobs.delete(jobId);
    this._freeWorker(worker);

    // Process next job in queue if any
    this._processQueue();
  }

  /**
   * Handle worker error
   */
  _handleWorkerError(worker, error) {
    console.error(`‚ùå Worker ${worker.workerId} error:`, error);

    const jobId = worker.currentJobId;
    if (jobId) {
      const job = this.activeJobs.get(jobId);
      if (job) {
        clearTimeout(job.timeout);
        job.reject(new Error(`Worker error: ${error.message}`));
        this.activeJobs.delete(jobId);
        this.stats.totalJobsFailed++;
      }
    }

    // Worker might still be functional, try to free it
    this._freeWorker(worker);
  }

  /**
   * Handle worker exit
   */
  _handleWorkerExit(worker, code) {
    console.warn(`‚ö†Ô∏è Worker ${worker.workerId} exited with code ${code}`);

    // Handle any active job on this worker
    const jobId = worker.currentJobId;
    if (jobId) {
      const job = this.activeJobs.get(jobId);
      if (job) {
        clearTimeout(job.timeout);

        // If not shutting down, re-queue the job
        if (!this.isShuttingDown) {
          console.log(`‚ôªÔ∏è Re-queueing job ${jobId} after worker exit`);
          this.jobQueue.unshift(job.jobData);
          this.stats.currentQueueSize = this.jobQueue.length;
        } else {
          job.reject(new Error('Worker exited during shutdown'));
        }

        this.activeJobs.delete(jobId);
      }
    }

    // Remove worker from pools
    this.busyWorkers.delete(worker);
    const availableIndex = this.availableWorkers.indexOf(worker);
    if (availableIndex !== -1) {
      this.availableWorkers.splice(availableIndex, 1);
    }
    const workerIndex = this.workers.indexOf(worker);
    if (workerIndex !== -1) {
      this.workers.splice(workerIndex, 1);
    }

    // If not shutting down, create a replacement worker
    if (!this.isShuttingDown) {
      console.log(`üîÑ Creating replacement worker for worker ${worker.workerId}`);
      this._createWorker(worker.workerId);
      this._processQueue();
    }
  }

  /**
   * Handle job timeout
   */
  _handleJobTimeout(jobId) {
    console.error(`‚è±Ô∏è Job ${jobId} timed out`);

    const job = this.activeJobs.get(jobId);
    if (!job) {
      return;
    }

    job.reject(new Error('Job processing timeout'));
    this.activeJobs.delete(jobId);
    this.stats.totalJobsFailed++;

    // Find and terminate the worker handling this job
    for (const [worker, currentJobId] of this.busyWorkers.entries()) {
      if (currentJobId === jobId) {
        console.warn(`‚ö†Ô∏è Terminating worker ${worker.workerId} due to timeout`);
        worker.terminate();
        break;
      }
    }
  }

  /**
   * Free a worker and make it available for new jobs
   */
  _freeWorker(worker) {
    worker.isAvailable = true;
    worker.currentJobId = null;
    this.busyWorkers.delete(worker);

    // Only add back to available pool if worker is still in the workers array
    if (this.workers.includes(worker) && !this.availableWorkers.includes(worker)) {
      this.availableWorkers.push(worker);
      console.log(`‚úÖ Worker ${worker.workerId} freed (available: ${this.availableWorkers.length})`);
    }
  }

  /**
   * Process the next job in the queue
   */
  _processQueue() {
    if (this.jobQueue.length === 0) {
      return;
    }

    const worker = this._getAvailableWorker();
    if (!worker) {
      return;
    }

    const jobData = this.jobQueue.shift();
    this.stats.currentQueueSize = this.jobQueue.length;

    console.log(`üì§ Processing queued job ${jobData.jobId} (remaining: ${this.jobQueue.length})`);
    this._assignJobToWorker(worker, jobData);

    // Process more if possible
    if (this.jobQueue.length > 0 && this.availableWorkers.length > 0) {
      this._processQueue();
    }
  }

  /**
   * Get current pool statistics
   */
  getStats() {
    return {
      ...this.stats,
      poolSize: this.poolSize,
      availableWorkers: this.availableWorkers.length,
      busyWorkers: this.busyWorkers.size,
      activeJobs: this.activeJobs.size,
      currentQueueSize: this.jobQueue.length
    };
  }

  /**
   * Gracefully shutdown the worker pool
   */
  async shutdown() {
    if (this.isShuttingDown) {
      return;
    }

    console.log('üõë Shutting down WorkerPool...');
    this.isShuttingDown = true;

    // Reject all queued jobs
    while (this.jobQueue.length > 0) {
      const jobData = this.jobQueue.shift();
      const job = this.activeJobs.get(jobData.jobId);
      if (job) {
        clearTimeout(job.timeout);
        job.reject(new Error('WorkerPool shutdown'));
        this.activeJobs.delete(jobData.jobId);
      }
    }

    // Wait for active jobs to complete (with timeout)
    const shutdownTimeout = 30000; // 30 seconds
    const shutdownPromise = new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        if (this.activeJobs.size === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 100);

      setTimeout(() => {
        clearInterval(checkInterval);
        resolve();
      }, shutdownTimeout);
    });

    await shutdownPromise;

    // Terminate all workers
    const terminatePromises = this.workers.map(worker => {
      return worker.terminate();
    });

    await Promise.allSettled(terminatePromises);

    console.log('‚úÖ WorkerPool shutdown complete');
  }
}

module.exports = WorkerPool;


